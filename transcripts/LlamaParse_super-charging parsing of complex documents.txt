 Hi everybody, I'm Laurie, DevRel at Llama Index, and I'm here today to demonstrate some of the features of Llama Parse, our industry-leading PDF parser and other document type parser. Llama Parse is really simple to use. It's an API that's built into Llama Index. So let's look at how you use it in this notebook. First, you pip install Llama Index and Llama Parse as two separate packages. And now let's get a really complicated PDF that we can look at and see how the features of Llama Parse come into play. The one that we've picked for this example is an insurance policy from the IRDIA. It's got lots of features. It's got lots of clauses and exclusions and coverage that it provides and doesn't provide. And it's a really great example of how you can use an LLM to simplify your life by turning this pile of really dense text into a summary of what it actually means for you. So what we do in this cell is we download that policy. And then in this cell, we're just initializing the async things that you need to get the policy. So we're going to get a notebook to work. And in this one, we are just pulling in two API keys. We need the OpenAI API key to do parsing. And we need our Llama Cloud API key, which you can get at cloud.llamaindex.ai to use Llama Parse. So we're going to use Llama Index.ai. Sorry, OpenAI's text embedding 3 small model. And we're going to be using GPT 3.5 Turbo as our embedding model and our LLM models respectively. So let's just look at how you parse things with Llama Parse at all. You import Llama Parse and you tell it what result type you want. In this case, we want Markdown back. And you give it that policy PDF that we just downloaded in the earlier cell. It parses that pretty quickly. And if you print it out, what you see is Markdown that represents the exact text that was in that PDF document. So we haven't changed anything. We've just turned it into Markdown, which LLMs find easier to understand. One of the things we can do is pass this Markdown into our Markdown. Element node parser, which will break it up into a set of text and tables. Tables are a big part of what makes complicated documents complicated, especially for LLMs. And by parsing it into tables specifically, we can get better results with our parsing. So I'm going to scroll past all the part where the parser was working. And the result is you get a bunch of nodes and table objects, which you can then pass into your vector store index as a simple list of objects. Then you can create a query engine from that vector store index. This is how you always use LLMA index. Let's see what happens when we query that basic set of text and tables. You get... You asked, my trip was delayed and I paid $45. How much am I covered for? And the result is unhelpful. You are covered for the amount mentioned in the certificate of insurance. That's not what we wanted. We wanted to know exactly how much we're covered for. The problem is that that information about exactly how much you'd be covered for is split across the document in a bunch of different places and the LLM can't find it. So one of the really neat things about LLM is that it's not just a set of tables. It's a whole set of tables. Now what we can do, and this is what we love about LLM, and what we love about LLM parse is that you can give the parser instructions. You can give the parser instructions that say this is the kind of document that I'm reading converted into something that is easier to understand as an LLM. So in this case, we give it a bunch of instructions that say, read this document full of policies and coverage and benefits and exclusions and turn it into a list, just a list of things that I'm covered for and things that I'm not covered for. And parse it that way instead. So now, having parsed it into two different forms, one is the just a pile of markdown and one is this list of coverages. Let's see what sort of responses we get. You can see that this is just printing out what the parsed form looks like. The original ones is still just a markdown conversion. So you can see that this is just printing out the version of the exact text of the document. But after the break line, you see what Lama parse has turned the same document into. It's turned it into this list of things that you are covered for, things that you are not covered for. Very simple set of statements that's easy for an LLM to understand. So now we put those things into a query engine. And let's run. The same query against our original vanilla query engine and our new query engine using the parsed form. Again, my trip was delayed and I paid 45. How much am I covered for? The vanilla version gives us the same answer as before. You were covered in the amount mentioned. But the instructions, the one that comes from the list, gives us the exact answer. You were covered for the delay amount you paid, which is 45. So we're not covered for the delay amount. Looking in the document, we noticed that one expense that was not covered is baby food. So let's ask the two query engines about baby food. I just had a baby. Is baby food covered? The vanilla version doesn't understand. It doesn't know. But the parsed version does. It says it has noticed the exclusion and said that baby food is not covered under the insurance policy. We can run a similar question about gauze. This is a very specific thing. Do you mention in your insurance policy how is gauze in your operation covered? And in this case, the vanilla version says it's covered as part of your surgical procedure. It gives this long answer that has all sorts of details. Whereas the parsed list version gives you a much simpler answer. Procedure charges cover the use of gauze in your operation. So you're fine. This is how you can use the power of LamaParse to, in effect, preprocess your complex documents to be simple enough that an LLM can usefully understand them and provide really helpful answers. So I hope this gives you some ideas of how you can use parsing instructions in LamaParse for your own applications. Cool. And I'll see you next time.