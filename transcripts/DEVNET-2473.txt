 And good morning, everybody. I hope you can hear me fine. Thank you for being into this session and welcome to Cisco Live. I am Alessandro De Prato and I am a technical marketing engineer for the data center and provider connectivity business unit. As you can guess from the title of this presentation, I work for the data center side of the company at the house and my group is the one responsible for the development of the Nexus, which is the ACI ecosystem, the Nexus dashboard and all the applications running on top of Nexus dashboard. I have two announcements today before we start. The first one is a service announcement. I just like to let you know that all over the Netherlands, the public warning sirens are being tested today at 12 a.m., which means I'm the lucky speaker who will have to stop for about one minute, one minute and a half. So when that happens, we're going to stop the session. We will wait for the siren alarm to ring and then we will start again. The second thing I'd like to tell you, today I will be the messenger of this session. The entire content and the pipeline definition itself was prepared by my colleague, Alejandro De Alda, who could not be with us for good reasons today, so I will be delivering it. What I would like to show you is an easy, safe, powerful, automated, consistent way that you could implement in order to apply changes to your network infrastructure with the help of ACI as the data center fabric technology, Nexus dashboard insights as the day two operations and obviously CI-CD pipelines for managing the workflow. I have two questions for you now. The first one is, have you gotten to the situation where the network was broken due to a change? Could you just raise the hand if that happened? Okay, that's fair. That's pretty much what I was expecting. And now all of you, how many of you think that the change, sorry, the outage, the fault could be avoided with a better testing or with a proper testing? Okay, good, good. And this is going to be the focus for the next 44 minutes. This is the agenda, right? So we're going to start discussing what the CI-CD pipeline is, very high level. We are not going to go too deep into this. Then I will tell you why it is important to always perform pre-change validation and post-change verification and we will see how Nexus dashboard insights can help us here. And then I'm going to run this demo where I will show you the entire pipeline definition, how to configure it, and we're also going to see that running. Before I forgot, remember that there is the Webex Room dedicated to DevNet 2473. You can join. I'm going to share the slides. I'm going to share additional links after the session is ended. If you have any questions for me, please feel free to ask them there. I will be more than happy to answer them over the next weeks as well. I will be keeping monitoring the room for a couple of months at least. Good. So what is a CI-CD pipeline? We can start from the acronym. The acronym stands for Continuous Integration, Continuous Deployment. You can consider it like an operational model that we imported from the software development industry. In a glance, you have to see a CI-CD pipeline like a list of tasks, heterogeneous tasks, automated tasks that are there to accomplish a goal. What is that goal? Well, it depends from your needs. You can achieve pretty much any kind of goal as long as you can automate those tasks. Today our goal is going to be to deploy something similar to this pipeline that you see defined here at very high level into the context of network infrastructure as code. So what we will be doing basically, we are going to cover all these jobs here. We are going to build the configuration. We are going to test the configuration to ensure that the configuration is not actually going to create any harm into our environment. If the tests are going to be successful, we are going to push that configuration in production and finally we are going to perform an end, a final verification. Now you might be wondering, okay, I could do all that manually, right? And that is true. I mean, I'm pretty sure that many of you here created maintenance plans, maintenance changes, documentations. I did it too and Tom there can confirm this. So that is true. But what you have to understand is the fact that pipelines, in general, automated pipelines are very consistent. They will never miss a step. And remember that two important pillars for infrastructure networks, and we can call them safe infrastructure networks, scalable, flexible, robust infrastructure networks, two of the main pillars are consistency and automation. Without consistency, we will end up in a network fragmented, different configurations templates. It's going to become harder and harder to maintain and troubleshoot different naming conventions, you name it. Without automation, despite possible, we know that managing the services, the platforms, the applications is going to take much more time compared with doing it with automation. And CI-CD pipelines bring a lot of benefits. For sure, they increase the efficiency and they decrease the overall deployment time. So they save many hours. They are very flexible. You cannot remove tasks depending on your needs. And if you have the proper tools, the proper testing tools, and if you design this pipeline correctly, you will also be able to ensure that your changes into the network infrastructure are not going to cause any problems. As an example, this is a very simple CI-CD pipeline that we can define at high level. We start from the operator point of view. Why is that? Because we are talking about network infrastructure as code. So it's something that is defined through code. So the operator, which could be the network administrator, the network architect, whatever, is going to apply the changes into the local repository. He's going to commit those changes and push them to the remote shared repository. At this point, here you see I'm using GitHub, but you could use other tools like GitLab and so on. The pipeline is going to be triggered. So all the jobs are going to be executed. The first thing we are going to do is take a snapshot of the infrastructure. And we do that through Python. Why? Because if we need to roll back quickly, we have the snapshot ready. The second thing, we are going to deploy the changes using Ansible. And finally, we are going to send a notification with Webex. You see we have different tools here. This is why I was telling you that it's very heterogeneous when it comes to the different tasks. But is there anything missing here? Something that we talked a little bit into the previous slides. We are missing verification. Exactly. Verification is extremely important, especially nowadays where the enterprise's revenues are really related also to the uptime of the network, to the uptime of the application and the services. So performing pre-change and post-change verification or validation is extremely important. And you see the number here. 65% of the incidents are caused by change activities. And this is not something I'm telling you because I made up the number. It's coming from an external research, right? And pretty much is what I saw when I asked you that question at the beginning of the session. Luckily for us, we have Nexus Dashboard Insights here, which can help us when planning or preparing changes into our network infrastructure. Why? I will tell you in a little bit. I just simply want to remind you what Nexus Dashboard is and what Nexus Dashboard Insights is. So we have to see Nexus Dashboard as an underlay platform. On top of it, once you have that cluster ready, on top of it you can install additional applications that are going to help you into the data center lifecycle at various stages. We have applications for day zero and day one, like Nexus Dashboard Orchestrator, Nexus Dashboard Fabric Controller, and Sun Controller. We have applications for day two stages for those phases. These are Data Broker, Nexus Dashboard Data Broker, and Nexus Dashboard Insights, which the focus for this presentation. Nexus Dashboard Insights is what we like to call the Day Two Operations Tool. It comes with many different functionalities, with many different features, and they all help you maintaining your data center once it is running in production with all the services already deployed. We have so many capabilities, so many features. We like to divide them into three main groups, visibility and monitoring, analytics and correlation, and finally, advisories and tools. We have capabilities for assurance, for compliance, for troubleshooting, proactive troubleshooting, reactive troubleshooting, you name it. Whatever you need for Day Two Operations is here. Today we will be focusing on the assurance part, and we are going to see the data analysis and the pre-change validation, which are going to be the key of this pipeline. Before we move on, how are those two features going to work? What you have to know is the fact that Nexus Dashboard Insights periodically takes the snapshots of the fabrics. Snapshots are data models. They contain information about the fabrics. What kind of information? Well, they contain information about the running configuration, the operational status, statistics about endpoints, about external routes, about deploying networks. They also contain information about the anomalies that are active into the fabric at that specific time. Now, with that analysis, what we can do, we can take two snapshots and we can compare them. Nexus Dashboard Insights will be able to tell us with a diff functionality if into the latter snapshot, for example, we had many more anomalies or additional anomalies compared to the earlier snapshot. That could be an indication that something is going wrong into our fabric. Also if we can compare that with the number of endpoints, number of external routes, and so on. Pre-change analysis works similarly. The difference is that here we are also considering additional changes that we would like to push into our ACI fabric. So we start from a base snapshot, we duplicate it, we make a deep copy of this data model, and then we apply on top of it the configuration that we would like to apply into the fabric. Through some machine learning algorithms, we are basically able to simulate what is going to be the status of the fabric if those configurations were about to be pushed. That will tell us if those configurations are going to raise additional anomalies, if they are going to fix existing anomalies, and so on. So we can have a nice view on how our fabric is going to look like even though those configurations have not been pushed yet. Now if we want to transform this into a pipeline, considering only the verification stages, it will be pretty much similar to this. The network operator will have to prepare the changes. You know with ACI you can prepare the changes into an XML file or JSON file. It will contain objects that are going to be removed, deleted, modified from the ACI management information tree. And we are going to do the same here. Once we have the file, we are going to provide it to Nexus Dashboard Insights. We are going to ask Nexus Dashboard Insights to perform a pre-change validation of it. At this point, Nexus Dashboard Insights will tell us, hey, there are going to be new anomalies triggered from this configuration. So you better go back to square zero and ensure or review the changes to ensure that this is not going to happen. Or hey, all good. The anomalies are not going to increase. So at this point, we can apply the changes into our production APIC. What is it left? The final verification, right? We also want to run the delta analysis between the earlier snapshot and the snapshot that we're going to collect after we implement the changes. You might be wondering, why do we need that? We already have the pre-change validation. In reality, the pre-change validation, you always have to remember that is a simulation. As an example, we cannot take in account if there is a bug into that specific code you're running that will prevent some configurations to be pushed on the devices. That is something that we cannot see with the pre-change validation. But with the post-change validation, you will be able to spot if something is not working as expected because it will raise additional anomalies. And you will be notified about that. So at this point, once we have run the post-change validation, we will see if we have new anomalies or if everything is green and we can call the maintenance done. Now, how can we build such workflow, such pipeline? These are the tools we are going to use today, but you are not limited to them. For example, for the continuous integration engine, we are going to use GitHub Actions. GitHub is not only a version control system. There are many other functionalities that you can use from it. Then we are also going to use Ansible. Ansible, we will use it for different reasons. First of all, we are going to maintain our infrastructure as code through Ansible playbooks and Ansible variable files. But we are also going to use Ansible to connect to Nexus Dashboard Insights and perform some activities in Nexus Dashboard Insights because what we have released is a collection, an Ansible collection, that has different modules to interact directly with Nexus Dashboard Insights. As an example, pre-change validation is going to be a matter of creating a playbook, same for the Delta analysis. Like I said, you can be very flexible. You are not forced to use GitHub. You are not forced to use Ansible. Nexus Dashboard Insights instead, it's the tool you want to use for the pre-change validation because we can offer that capability. This is going to be our target pipeline. So for the last time, you will see the file at high level. Then we are going to see how to implement it in a matter of workflow configuration. We always start from the operator. Remember, the operator pushes the changes into the remote repository and at that time the pipeline is going to be triggered. The first thing we are going to do, we are going to validate and link the playbooks, the variable files that we are pushing into the remote repository, just to ensure that they don't contain any semantics error, any syntax error, and so on. Then we are going to run our Ansible playbook. But the cool thing is that we can run the Ansible playbook in dry run mode. It means that we are not going to push those objects that we have defined into our playbook into the APIC. We are simply going to create a file, a file that will contain all those objects defined locally. Once we have the file, you can guess, we are going to pass it to the pre-change validation functionality of Nexus Dashboard Insights. And at this point, we are waiting for a result. Nexus Dashboard Insights will either tell us, go, right, you don't have any new anomalies, or no go. You have new anomalies, so be careful because if you push those changes, you might break your network. If we are lucky and we don't have any new anomalies triggered by our changes, we will continue with our pipeline. We will take a snapshot using Python and then we are going to run exactly the same playbook we did before with Ansible, but this time we are not going to use the dry run mode. This time we are going to push those objects into the APIC. And finally, once more, we are going to use Nexus Dashboard Insights to perform a post-change verification. So we are going to collect a final snapshot, and that snapshot is going to be compared with the snapshot that we took before when we run the pre-change validation, which was the initial status of our fabric. So by doing all this, we will be able to understand if our fabric is stable or if we have new issues that need to be investigated. Finally, we are going to send a web ex-notification, but as you will see from the pipeline definition, we actually send a web ex-notification in every job, in every task, just to update the network administrator, the network operator about the status of all of them. Cool, demo time. So I will start by showing you the actual pipeline definition now. I hope you can see it. Yes, it looks good. We are talking about GitHub actions. The good thing is that you can define your pipelines in a YAML file, and the YAML file is going to be contained into the actual repository. So here you see I have my Visual Studio Code just to review the files. I have the .github.slash.worldflows directory. Inside of it, I have the validate and deploy YAML file. A few initial definitions that we have here. The first one, the name. Where are we going to see the name? Well, we are going to see it into GitHub dashboard. We are going to see it into the logs wherever we are mentioning this pipeline execution. Then the interesting part on what does it mean? When do we want to execute this pipeline? Here we are telling GitHub that we want to execute it every time there is a push or a merge into the main branch. So every time we commit a new change into the remote repository, that workflow is going to be activated. We can set some environment variables like this one to avoid those warnings from Python. And then we have the jobs section. The jobs are all the tasks that we want to run. Here we have, let me just shrink this one, we have six different jobs. Now by default, they will all run in parallel. But as you can imagine, this is not something we want to have into this case because we have a specific timeline. We want first to create the file, then we want to analyze the file. If the testing is good, we want to push it. So I will show you in a little bit how you can define the order between the different jobs. Let's open first the initial job, AnsibleLint. Runs on. This is an interesting one. We want to run some sort of automation. So we need CPU. We need RAM. We need somewhere where we want to run those applications. We have two options. We can run it on some VMs that are provided by GitHub itself. You have a free tier available. If you consume too much space, you will have to jump to another tier where you have to pay something for utilizing them. Or the second option is that you can run this sort of tasks on your VMs. So you deploy a new VM into your on-premises data center, wherever you want. You install the software provided by GitHub, right? And then this VM will connect to the GitHub cloud and will stay there waiting for instructions. You do not need to open any inbound connections. Everything is going to be controlled by an outbound connection, direct or indirect via proxy. You can choose. In this case, we are running this task on the GitHub provided VM. The next thing we are saying are all the steps that we want this job to execute. So as an example, we are setting up Python into the VM. We are installing Python version 3.8. Then we are going to install the Yamalint module into Python, and we are going to execute that module into the Playbooks folder contained into our repository. In this way, we are going to verify if all those files are correct. The next thing we are going to do is to send a Webex notification just to report the status of the job, the execution. Take a look at this. If always. This condition is required because if any step will fail at some point, the job is going to be interrupted and the pipeline will fail. But we don't want that. We want always a notification to be sent, even if one of the previous steps is going to fail. And that is why I have that condition there. On top of this, you see we can also define some or we can also enrich, let's say, this pipeline with some variables or secrets that are going to be defined into GitHub Vault. And I will show you later on where you can find those settings. Now let me collapse the AnsibleLint. The next thing we are going to do is the dry run. This is the keyword needs AnsibleLint. You have to use to specify the order. Because now we are telling the workflow that before it runs the Ansible dry run, it will have to wait for the AnsibleLint job to be completed. What else are we going to specify here? Runs on self-hosted. This time we are going to use the runner we have into our premises. And we're also going to tell GitHub to install a container. This time it's going to be a custom Ansible container that we want to run into that VM. And then it's going to be a matter of performing the steps, right? One thing that maybe I forgot to tell you before, you see this uses action checkout. This is telling the job that it has to download the entire repository. Because we want to work on those files. We need to see those files. We need to evaluate those files. Then we are going to run the playbook. And the playbook name is going to be deploy.yml. Mind this. Minus, minus, check. This is the important thing I was telling you before. If I'll open the deploy.yml playbook, you see that it's simply importing additional playbooks here. So if we want to see what they are about, we can open one of these. And there we go. We see that here we are defining tenants. We are defining VRFs, bridge domains, and so on. One important variable is this one. Output path. This is the file name we are going to generate by using the minus, minus, check option. Remember, we are not going to push those configurations into the APIC. We are simply going to create the file. Let's go back now to our pipeline. Once we have run the playbook, we will basically obtain the dry run data file and we will upload it. We are going to upload it as an artifact to the pipeline itself. Why? Because remember that as soon as this job is completed, the container is going to be destroyed. So if we don't save the file somewhere, we will lose it. We are going to use the artifact action to upload it temporarily to the pipeline execution. It can offer some sort of storage. And then we are going simply to send a notification with WebEx once more. The next thing we are going to do is going to be the pre-change validation. So always we run this on the on-premises because it's very true that we don't want to expose our Nexus dashboard insights from the cloud. We want to have it protected so our on-premises runner will have connectivity to the Nexus dashboard insights. What else are we going to do? The same things. We are going to download the container, the Ansible container, we are going to download the repository and we are going to download the artifact, the file that we uploaded into the previous job into the GitHub pipeline. Once we have that artifact, that file, we are going to run an additional Ansible playbook. And this time the playbook is going to work with NDI. It's going to take that file, the artifact, and it's going to pass it to Nexus dashboard insights for running the pre-change validation. We can take a look at the actual playbook. It's in the Tools, Change, Validation. And by the way, this entire code is available in GitHub if you want to review it later on. So if you're missing something now, don't worry. It's going to be there. We also have an appendix to the slides where you can take a step-by-step look on how to build it. So the pre-change validation playbook is this one. We are going to connect to Nexus dashboard and then we are going to pass, there we go, the dry run data JSON file once more to the pre-change validation module. This module will simply request a pre-change validation job and then the playbook will wait for that job to be completed and it's going to evaluate it, meaning that it's going to perform the diff between the previous snapshot and the latter snapshot. Now let's go back to our validate and deploy definition. I'm going to shrink this one and the next thing we are taking a snapshot. Let's assume obviously that the validation was a success, right? We're going to take a snapshot. This time we're using always into our on-premises runner a different container. This is a simple container that simply runs a Python script using the ACI Cobra SDK library. Nothing more. I could have done it with Ansible. It was just a way to show you that you can use whatever you want as a tool when it comes to automated tasks. So it's going to be just an easy one and once we have took the snapshot, we are going to deploy the changes. Now take a look at this. The name of the playbook is exactly the same as before. We still have deploy.yml. The only difference is that now we are not specifying minus minus check, which means that we are actually going to push those configurations into the APIC. We are not going to generate the file anymore. If that goes well, it means that we are ready to run our final job, the post-change validation. What we are going to do here through again a new playbook, which is called post-change validation, is very simple. We are going to connect to Nexus Dashboard. We are going to gather some information about the pre-change validation we run before in order to see what was the base snapshot. We are going to trigger a new snapshot on our fabric to collect the latest information. We want to see how the fabric is doing now. And once we have that, we are going obviously to wait the snapshot to complete, but then at the end we are going to create a delta analysis and we are going to evaluate that. At this point, if we get a positive message, it means you don't have any new anomalies. If you have a failure, it means that we have additional anomalies that we have to investigate. And that's it. This is the pipeline definition. Now what I want to show you is the GitHub side of the things. So this is the code section you can have in GitHub, and it's exactly the same what I was showing you in Visual Studio Code. Now the important things that I wanted to show you over GitHub are in the settings, the first one. So if I move to the settings section, you see that in Actions, Runners, here is where I can install or I can link my runners running on the on-premises. I have one, and you see the status is idle because the software is running. It is connected to the GitHub cloud, but it has nothing to do because GitHub has not any running pipelines executions, so it's just sitting there and waiting for instructions. How can you install a new runner? Well you go to this section, it's very easy. You click on this green button, and they will provide you all the commands you have to do to you have to apply to download the code, to download the software, install it, and eventually configure it. Then you can obviously customize this. You can run it on demand, you know, just executing the binary. You can create a service and run the service however you prefer. The next thing that I already showed you a little bit before, it's the Vault. You have two options when it comes to the Vault. You can define secrets here or variables. You can guess the difference, right? Secrets are about sensitive data, something that we don't want to show once we have defined it. You don't see the values of the secrets here. You won't see the values of the secrets into the logs as well. When it comes to variables instead, it's the exact opposite. We can see the values of the variables, and we are also going to see the values if the variables are debugged into the logs. And now you might be asking, where are these logs? There is one more section that we have in GitHub, which is the action section. And you see here that I have the logs for two pipeline executions that run into the past day. Obviously there were hundreds of them all failing, but I had to delete them just to clean up the stuff, right? So the first one, add new application, it failed. Where did it fail? Can you see it? Good. Let me zoom it a little bit. Yes, it failed during the pre-change validation. So we had a pass on Ansible Lynch, we had a pass on Ansible Dry Run, we failed the pre-change validation. If we click on pre-change validation, you see that we have the entire job history logged here. And GitHub is going to open the exact section where we can see the failure that happened into that specific job. Here it was a failure about the pre-change validation itself, meaning that we had or we obtained additional anomalies running on the pre-change validation. Now I will show you this over the demo a little bit better, because you can realize that from the logs it might be harder to understand what actually is happening on the pre-change validation. Obviously you will have to use that in conjunction with Nexus Dashboard Insights, because Nexus Dashboard Insights has the web UI that nicely displays all this information. Going back to the summary, you see instead we go to this one, the fixed change, where the entire pipeline was completed, and you see all green lights. One more thing I want to show you before we start with the actual demo is down here. If I scroll down to the bottom of this page, you see that we have the artifacts section. The artifacts is what I was telling you before. The place, the storage where we are going to take the files we are generating when we run the Ansible dry run, we are going to upload it here, and then the next job, the pre-change validation, will download the file and it will request a pre-change validation job to Nexus Dashboard Insights. What is going to be our scenario for the demo? Just one thing. The entire pipeline might take several minutes, depending on what is the size of the configuration, what is the environment, meaning how many switches you have running into your ACI fabric and so on. I had to record the entire pipeline execution. I had to shrink many sessions, because I think otherwise it would have taken 50 minutes to show some failures and everything. This is going to be our scenario. We are going to start with this overlay configuration. One Prod PCI tenant and one Prod non-PCI tenant. They are both defined as overlay policies in ACI. There is one interesting thing that you can see here, the fact that we are using a shared layer tree out in order to provide external connectivity to both tenants. That means that all the subnets for the PCI and the non-PCI will actually coexist into the shared VRF. What's our goal? We want to deploy a new application profile. The new application profile is going to contain new EPGs, it's going to contain new contracts, it's going to contain new filters and so on. And obviously we are going to run this against our pipeline. Let me move now to the demo. And let's start. We are now into the ACI side. You see that we don't have application profiles, we don't have contracts, we don't have filters. Everything is pretty much empty as we are expecting because we want to push that new application profile. So we go into our code, remember we define everything through code. We import the additional playbooks. What are these playbooks going to contain? Well we saw it before, right? Variables and configurations for ACI. We are adding the new playbooks into the deploy.yml which is the master one which imports the additional. And there we go. It's pretty much the same thing I showed you before, right? It's exactly the same playbook. We are defining the new application profile, we are defining the contracts, the bridge domains, the EPG static bindings. You see that we are simply looping through some objects because the objects are actually defined into the variable files. We might have multiple EPGs, multiple filters. So we read that variable and we loop through the different classes. Now that we are happy with this, it's time to commit the changes. So we are going to provide a meaningful message into the commit that we will deploy HRMS up and once we have the commit, we are going to push it to GitHub. Now we move to the GitHub side. We go to the dashboard we were seeing before and if we refresh the page, we are going to see the commit there and it's executing now. The first thing is going to be the Ansible lint. Remember this is going to run on the GitHub VMs but at the same time on the side of the screen we are opening a connection to our on-premises runner just to see what is going to happen. There we go. The Ansible lint process is completed and now we are running the dry run on the on-premises side. Remember the dry run is going to execute those playbooks, is going to look at those variables and is going to generate a file. We are not going to push those changes into the production APK yet. These are the objects that we want to create. You see them through the logs. But if we move to the APK side, you see that we are not yet seeing anything here and this is expected. So now as you remember, we are going to wait that job to be completed. We will take the file. There we go. It was a success. We are going to take the file. We are going to upload it as an artifact and then into the next job, the pre-change validation, We are going to download the artifact and we are going to ask Nexus dashboard insights to perform a pre-change validation. And there we go. You have the logs now. We can go to Nexus dashboard insights and see what's happening there. We have a new job running. That's the status. And if you take a look at the name, it could look like a random string. It is not a random string. That is the git commit ID. It is just to have a reference so you know what actually triggered that pre-change validation. Now I'm fast forwarding this session. You see that the pre-change validation failed. It took 12 minutes. We get the message over Webex like we do for any other job. And if we go there, we can inspect the logs of the job itself. We see that we are going to trigger two new anomalies, one critical and one major. We could take a look at here over the logs, but it's pretty difficult to understand. I mean, you need experience. So what we could do instead, we go to Nexus dashboard insights. We open the Delta analysis section of the pre-change validation. And there we go. Through these Venn diagrams, we can understand, looking at the right circle, if we are going to have any new anomalies for any kind of severity. We have critical, major, and warning. In this case, we are going to have one new anomaly for the critical severity and one new anomaly for the major. When we get additional details, of course, we click into one of them. And as you can see, the first anomaly is going to be a traffic restriction compliance violated. Let me stop one second. What is a traffic restriction compliance violated? With Nexus dashboard insights, you can define what we call it compliance rules, or even better, communication compliance rules. You are able basically to sell Nexus dashboard insights. Hey, you know what? These two EPGs should never talk between each other. Or these two EPGs can talk between each other only using a specific port. Now Nexus dashboard insights will always evaluate that. And if the condition is not met, is not matched, it will raise an anomaly. This is true for the live environment, but it is also true for the pre-change validation. Because basically here, we can detect in advance this failure. Even if those changes have not been pushed yet to the fabric. So we can tell the user, hey, if you are going to push these changes, you are going to break the validation, the compliance validation. The second one, the major one, is an overlapping subnet across bridge domains into the same VRF. Remember that we're using a shared VRF, a shared layer 3 out. And that means that we are going to merge different subnets into a shared bucket. In this case, we probably used the wrong bridge domain association into our code. So we are using the same subnet on the PCI tenant and on the non-PCI tenant. And that is going to cause a problem. We know if you use the same subnet, things are going to be broken. And Nexus dashboard insights is able to tell all this and is also providing us details about what is the bridge domain that is causing this, where the overlap is going to happen and so on. So what can we do at this point? Well, pretty much nothing. We have to go back into our code knowing what's going on. And we have to fix the code in order to prevent those faults. So we go to our variable section. We are going to remove the contract association that was creating that unexpected communication. We are also going to change the subnet of the bridge domain. Or better, we are going to use a different bridge domain relation for all our EPGs. So the 20 subnet is not going to be included. Next we proceed as we did before. We commit these changes, we provide a commit message, and we push the changes into the remote repository. So we can move back now to the pipeline, to the dashboard, and we see that we have the new pipeline executing. Things are going to be exactly the same as before. And this is why it's good, right? It's consistent. The CI-CD pipeline are consistent. They will never miss any point. So fast forwarding, you see we are doing the Ansible lean, we are doing the Ansible dry run. Now we are running the pre-change validation. And this time the pre-change validation was a success. We can take a look at the logs. We can scroll down. And the logs are telling us, OK. That is the message that we get from the Ansible module that is running the pre-change validation. If we open the Delta analysis now on the pre-change validation side, there we go. 0, 0, 0. No more anomalies or no new anomalies. So we are OK to continue. We are going to take the snapshot, obviously, and now it's time to push the changes. We run exactly the same playbook, this time without the minus minus check. So the objects you see now through the log, like these subnets, these bridge domains, are actually getting created on our APIC. We can quickly switch the view to the APIC. We can take a look at the contract. You see that we have values now before they were not these objects. The filters are popping up as we speak. We have ICMP, HTTPS. And we can also take a look at the application profile, which got created now. Into the topology, we hit refresh. And we see that all the objects are defined with all the associations with the contracts, the relations between the different EPGs. This is pretty much what we are expecting. And indeed, the deploy phase is completed. It's time now for the final one, the post-change validation. You remember what we do here? We query information about the pre-change validation to have the earlier snapshot. We trigger a new snapshot to get the information about the fabric at the current status. And then we run the delta analysis between them. This is something that we can follow also. Actually, this is where you can see that the assurance analysis is running. So this is the place where we can see that the snapshot is getting collected. Yes, there we go. So you see that the post-change validation is running. So now we are analyzing the two snapshots. We can take a look at this over the logs, like we did before. We can also take a look at this over Nexus dashboard. There we go. We open the logs. We are reporting everything that happened over the Ansible playbook. And at the end, it was a matter of trigger the delta analysis and validate the delta analysis, which we can basically also inspect from Nexus dashboard insights. This is our delta analysis. And it's basically telling us, hey, you don't have any new anomalies after you committed those changes into the live fabric. So our pipeline is basically deployed entirely. We have all green lights. And our network is safe. Now, it's time for the conclusion part. I think the key topics for this presentation are the fact that changes can be dangerous if they are not performed into the correct way. We saw the numbers, right? 65% of outages of problems happen actually during the changes. Luckily for us, we have the tools that can help us here. The first one is the CI-CD pipeline. Because you saw CI-CD pipelines are consistent and are automated. You can and you will be sure that all the tasks are always going to be run into the most precise way. And obviously the second tool is Nexus dashboard insights. Because Nexus dashboard insights can provide to the pipeline those testing functions, those validation verification functions that we need to add to our pipeline to ensure that things are going to run smooth. I just want to leave you with this slide here. You can download the slide from the Cisco Live app. You can download the slide from the Webex room. I will post them there. You will find the link to the code in GitHub. You will find the link to the sandboxes that we have available into the DevNet portal. If you want to play a little bit with something like this, something similar. There are also a couple of more sessions happening into the next days that are covering pretty much the same topics. Maybe with Terraform, maybe different declinations. But overall the technologies are going to be these. Obviously I'd like to ask you if you have a minute just to fill the survey and let me know how you felt about this session. If you like it. If there's anything that we should, let's say, improve in terms of presentation, in terms of content on the slides. It's going to take you just one minute. And remember that Cisco Live just started today. There are plenty of activities that you can do around there. We have DevNet sessions. We have breakouts, labs, D cloud sections where you can expand your skills. It's a great opportunity for this. So thank you very much for being into this session and I hope you will have a great Cisco Live experience. If there are any questions, feel free to raise them. I think we have one minute more. Thank you very much.