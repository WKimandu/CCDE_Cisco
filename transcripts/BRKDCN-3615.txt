Good afternoon everyone. Thanks for being here. I have the immense pleasure to be the last one standing between you and the party. Not sure if it's a pleasure or a curse, you'll tell me in an hour. So my name is Roland, I'm principal engineer working in the TAC in Brussels for many many years and I've been focusing in SEI for 10 years and we're here today to talk about PBR. PBR stands for policy-based redirect, not routing. It's really about redirecting the traffic based on contract and not too much about routing. This is a very wide topic, we could probably cover, if we want to cover all zillion features of policy-based redirect, we could probably spend four or five hours. So I've been deciding to focus on an introduction and focusing on a generalization of the policy-based redirect. So I'm going to start with the policy-based redirect generic packet load which is the simplest possible PBR scenario and then we'll take some use case which we see more and more. East-west PBR in a multi-pod, east-west PBR in a multi-site and finally we'll end up with multi-site any-to-any PBR which is a brand new functionality that was added officially in 6.04, so something like last month. So first, why do we need to talk about PBR and why is it important? Well, it's all about service insertion and mostly firewalls. So all my examples are related to firewall even if you can perfectly use it for load balancer. Without PBR, it's not easy to have a firewall in an ACI fabric, especially for east-west. Either you leave the default gateway on the firewall, which limits you greatly in your design, or you do VRF stitching with a VRF for the consumer, a VRF with the provider, but then you need two-arm firewall, which is an over-limiting factor. The routing is getting more complex, you need more VRF. It can work out, but that's not ideal. So PBR allows you to just insert it. So you still keep a contract. PBR is a contract. It's attached to a contract. You have a consumer and you have a provider, and you just drop your firewall in between and ACI will make sure that the necessary constructs are there to have every packet that goes between your consumer and your provider. So if you're matching your contract, just go to the firewall instead of going directly to the destination. So that's very convenient. In most scenarios, you can keep one arm. Even if you can do two arms, 99% of the deployment of policy-based redirect are done with one arm. So the routing on your firewall is extremely easy. You just need a default route. And your contract will be attached with what we call a service graph, which is how you can reach the firewall. So once you configure it, we're going to deploy a set of things. And it's all done for you. At the end, the firewall is still physically attached to your fabric, so it needs a VLAN to communicate to the fabric. Anything that is attached to the fabric needs to be part of an EPG. So that's going to be what we call a shadow EPG or a service EPG. Pick up whatever terminology you like. And that's the way we'll look at it. We will see what's the use of it. It's important for troubleshooting. You don't deploy it. It's automatically deployed while you run your service graph. We'll call connector the leg of your firewall or your service device. So you'll have a consumer connector, which connects between your consumer and your service device and a provider connector. If you run one arm, the consumer and the provider connector will end up to be the same VLAN, the same shadow EPG. If you run two arms, they're going to be separated. So the flow will be, even if you configure a single contract, will be like if it was a contract between your consumer to your firewall, and then a second one, which goes from your firewall to your provider. So we'll go twice through ACI fabric. Before digging into the example, let's quickly remind where the policy is applied in ACI. So even without PBR. The rule in ACI is that we will try to apply the policy in ingress if we can. And if we're not capable to do so, we're going to do in egress. So the ingress leaf will get a packet, and we'll need to determine the source EPG and the destination EPG PC tag, which is the identifier of the EPG. It's also called S-class and D-class. The source class, we can always derive. It's coming from where you receive the packet. You receive it on a specific VLAN and interface, that gives you the S-class. So that's enough. For the D-class, that's more complex. We might be able to find it if the endpoint destination is known in the endpoint table. In the show endpoint, it's going to contain the policy information as well. For layer three out, it's going to be part of a zoning prefix or a routing table. But in some scenario, we can't find it because we don't have that information. Then we're going to use some bit in the header, SPDP bit in the VXLAN header to make sure that the policy was not done yet. So when it's reaching the destination, the destination leaf will look, was the policy applied already? SPDP set to one means it was applied, and I can ignore the contract. I'm just going to forward the frame. Or it's not, and I need to do it, and then the contract will be done in egress. For regular contract, it's pretty invisible. For policy-based redirect, that plays a big role, and that adds some extra complexity in the possible flow we're going to add. So now let's dig into policy-based redirect. Very simple example. I have a consumer on the left, a provider on the right. I have a firewall in the middle. And obviously, I want to go through the firewall in between my EP1 and my EP2. So my EP1 sends a packet, a regular .1Q packet. It's going to reach the leaf. And you'll see here, I just put for your reference, we're not going to explain it to you. But if you're interested to make deep troubleshooting, if you know about ELAM, this is what you need to copy-paste, even without the ELAM assistant, to be able to capture the packet at that stage. You'll find that type of thing three or four times in the presentation for your reference. So leaf one will apply the contract. I'm assuming that my leaf one is capable to apply the contract as enough information. So what it's going to do is it's going to make a policy lookup, and instead of fitting, a rule that tells him permit or deny, the route will tell him redirect. So redirect means it needs to go to the firewall. So what we're going to do, the leaf will rewrite the destination MAC of the packet received. So the packet will now be sent to the firewall virtual MAC. This happens on the ingress leaf. It's going to then encapsulate it in the VXLAN header, and it will send it in a VNID, a VXLAN ID, which is the VNID of the VXLAN header. So it's going to send it in a VNID of the VXLAN ID, which corresponds to the VNID of the bridge domain where the firewall is. So at this stage, it's a layer two packet in the fabric. It's in a bridge domain VNID, and the destination MAC, it's already the firewall MAC. So the last thing we need to know is where is that MAC? Where is that firewall? Fabric might be larger. We will never make an assumption. We will not make learning on that MAC. We will always send it to the spine, to the anycast loopback of the spine, to let the spine look up in the Kube database. Kube database is the database on the spine which holds the knowledge of every endpoint in the fabric, and that includes the MAC on the firewall like it's including the IP of every endpoint. On the CLI point of view, what will it look like? So you need to find your source class and your D class. For an EPG to EPG, which is the flow I'm covering now, so east-west traffic, we're typically going to start by the endpoint table. For the source IP, we find the S class. We do the same for the destination IP. We find the D class. We have enough information to check the contract. We check the contract between our source and destination, and we see it's a redirect. We still don't know yet where we're going to redirect it, but that's a clue that we hit a PBR rule. Note the filter. Here I use filter ID. My filter ID one is just IP. It's very important when you do PBR to not use a command default filter. It will work in many scenarios, but if you ever make policy-based redirect within a layer two domain, a bridge domain, between EPG of the same bridge domain or within the same EPG, you have a good chance, if you use command default, to redirect the ARP traffic to the firewall, which will break your flow. So as a best practice, when you deploy a service graph with PBR, we usually recommend to avoid the default filter and to use something more specific. IPv4, everything is fine. Then we know we're a directory-specific group. It was grouped through in my case, so through service redirect info, tell me it goes to that VIP, 172.161.100. We can see what rewrite information we have. This last information highlighted in green and in red is exactly what we need to rewrite the packet. So that leaf has enough information to modify the packet to be a layer two packet in the bridge domain of the firewall to the destination virtual MAC of the firewall, and it will send it to the Anycast V4 of the SPINE. If you're a gig that wants to look at ELAM, if you succeed to get an ELAM via ELAM assistance or not, there will be a little flag in the very raw unreadable detail, which is called service redirect, which is called service redirect, which is called service redirect, that confirms that that specific packet you catch is actually also redirected, so which data path is confirmed. So I'm now at the SPINE, but I still don't know where is my firewall. So I'm going to ask the SPINE. The SPINE is supposed to know everything. So I'm going to ask the SPINE, can you check in the bridge domain of the service BD if you know where is the firewall? And it better knows, otherwise you're going to drop the packet. So again, if you ELAM, here it's an extract with ELAM assistance, you can confirm that we receive it to Anycast MAC SPINE proxy, you can confirm the PC tag, but what we care is the COOP information. So you can query the database based on a VNID and a MAC address, and it should have a tunnel interface. If the MAC address you saw in the redirect info is not there, either your firewall is dead and not sending anything, or you did a typo when you confirmed that the MAC address is not there, or you did a typo when you confirmed that the MAC address is not there, or you did a typo when you confirmed that the MAC address is inside your APIC. Majority of PBR deployment has a static configuration of the MAC address in the redirect policy. We'll see it in a moment. In recent code, you can discover it dynamically with IPSLA, but in case you don't do that, it's your responsibility to ensure the MAC address you configure on SEI match the virtual MAC address of your firewall. And if you change the firewall, unless it keeps virtual MAC, you'll need to change it. So if it's not there here, that's most likely either a firewall is down or a config issue on your APIC site. So we know it's behind LIFU. We go to LIF2. Here the job is easy. We're just going to layer 2 switch it. Again, ELAM if you're interested. So we're going to verify that the MAC address is known. Again, it's the MAC address of the firewall. I see it known with a PC tag. What could be that PC tag, by the way? Do you think it's a consumer? Who thinks it's a consumer EPG, the MAC address of the firewall? Who thinks it's a provider? Who thinks it's something else? What can it be? The shadow, exactly. That's the MAC address of the firewall, so we learn it in the shadow EPG. This internal VLAN refers to here VLAN 1417, which is the VLAN used to connect to the firewall. Goes to the firewall. It's coming back. Now it's like a new packet from ACI point of view. So source EPG, we have a guess. It's going to be the shadow. That's the same thing. Now the source MAC address is the firewall, so it's classified in the shadow EPG. The destination IP, it's still my destination server on the right, so it's going to be the EPG of the destination. So we'll find a second rule, which permits the traffic coming back. from the firewall to go to the destination. So we receive it in that MAC address, which is the MAC address of the firewall. This is the shadow EPG. The destination, we know it, and our zoning rule, now it's not a redirect, it's a permit. So the blue value represents the service EPG. So we can permit it to the destination, and it's finally reaching the final destination. This CLI contract parser, it's quite useful. Python script, you can filter it, you can use grep. Typically there are a lot of options per VRF, per destination EPG. You can use the name of the EPG instead of this bizarre number. It's going to tell you the rule of the contract, and at the end it gives you the hit count. So it should increase if you get constant traffic hitting that specific rule. The return flow is exactly the same. So I'm going to show you the return flow. So the return flow is exactly the same. So I'm going to show you the return flow. So I'm going to show you the return flow. not going to show you you're going to have similar rule in the return flow a redirect from the provider to the consumer and a permit from the service shadow epg to the destination so every pbr implementation no matter how complex it is will follow the same type of flow the leaves that make the redirect redirect the mac address makes a layer 2 packet to the bridge domain of the firewall and sending to the spine proxy to make one exception which i will not cover because not much people are using it it's in case your firewall destination is behind the layer 3 out so you don't redirect to an endpoint in the bridge domain you are direct to an ip address behind the router so this is the only exception in that flow now let's make it a bit more complex let's think about a multipod and we want to have multiple firewalls for some reason we want some redundancy in the firewall cluster so we want to have multiple firewalls for some reason we want some redundancy in the firewall cluster so i'll take an example where you have i'll call h a pair because i even if i only draw one firewall in pod one and input two it's typically a cluster in pod one and a cluster input two and i'm assuming the cluster don't talk together so these are independent cluster redundant by themselves but there is no communication for cluster one from cluster one to cluster b2 to succeed to implement that in multipod we need something called symmetric pbr symmetric because like any firewall it needs to get the traffic in both directions so if my endpoint on the left redirect to hfr1 to reach the destination the return flow should better go through hfr1 and not hfr2 or state won't be created there and packet is dropped so we need to ensure it's symmetric so symmetric pbr achieved that through a hashing of the packet so through config gotcha about configuring the redirect policy further is this slag enable pod id aware redirection we have a multipod that's a very appealing name i want to do it looks looks looks fancy looks nice you should never never do it if it's east-west traffic so if your provider and consumer or epg you should never use that option it's only for north-south traffic so only if your provider or your consumer is a layer free out you can use it If you do it and you apply it as consumer provider to EPG to EPG, you will get traffic asymmetry for some flow depending on the location of the endpoint. So it's a very small niche use case. The Enable Anycast, that's also a niche use case. It's only for firewalls which are active-active. So instead of having two separate HA pairs, it's one big firewall which has the same virtual IP in pod 1 and in pod 2. So we don't care about symmetry for the data path because the firewall synchronizes between them through a cluster link, the states. But it's a feature of the firewall, we will not cover it, but we absolutely support it in multipod today if you enable that flag. By the way, we don't support it in multisite. Next, the MAC address and the IP address. So this is the VIP we want to redirect to. Even if it's two separate firewall clusters, from ACI point of view, the redirect is a single function. I redirect to that firewall, the function firewall, no matter how many appliances are behind that. So if I have two VIPs, I specify two VIPs and the corresponding MAC address. So that's where I say if you make a wrong MAC address, the MAC address here, most likely it will never be learned in Kube, so all your traffic will be dropped. In 5.2, you have the option to leave that at zero, but you need to enable an SLA monitoring in ACI that will auto resolve the MAC address to the IP address. It's very handy, but a lot of people still do the manual way. Last, you have the hashing. So that's what the information we're going to take from the packet to decide where to send it. So by default, we hash source AWS IP and the protocol. So all packets from Server A to Server B using TCP will be hashinged the same way. UDP might be hashed differently. We don't want to hash further down than TCP because there are many applications that could have dynamic ports. So we want to ensure that if it's TCP between the twoção server, we hash the same way to avoid potentially a weird application be broken if we hash it directly. on the destination TCP port, for example. And why do we call it symmetric? Because, obviously, the hash is symmetric. So if A to B proto X give a certain hash value, B to A proto X will give the exact same hash value. So we ensure the traffic from A to B will take the same path in return. Of a gotcha on the config, when you configure your L4, L7 device, again, it's two cluster in different pod. However, from ACI point of view, that's seen as a single redirect information. So you'll configure on the top your two device and how it's reached the fabric. And you specify on the bottom a single cluster interface. Cluster interface is similar to a logical interface. That's how which VLAN, which stretch VLAN we use for the ACI. That's how which VLAN we use for the ACI to speak to the firewall. If all is that, now let's look at the path. It's very similar to my first example. The ingress leaf will try to do the redirect if it can, if it can find the D-class, exactly like my first example. The only difference is that in step two, we first make an hashing of our packet. And based on the hash, we're going to select one of the two possible virtual IP. For the rest, it's exactly the same. Once we selected which virtual IP we're going to redirect to, we rewrite the MAC address. We send it to the service bridge domain we need. And we send it to the spine for the spine to be able to tell it's actually located on leaf two. And the rest of the flow is the same. Coop look up on the spine, going to the leaf layer two, back from the firewall. There should be a permit from the service shadow APG to the destination. What check can you do? I didn't mention it in the first example. I should probably have done, but I do it here. Any time you deploy a new service graph, check number one, is the graph deployed? You go on the UI, you see in your deploy graph install, is my new graph properly deployed? If not, most likely, that's a contract issue. You don't have consumer. You don't have provider, something like that. If it's deployed, check if there is no fault. If it's no fault and deployed, most likely, you can jump to the CLI. Your graph is correctly deployed and redirect should work. However, before going to the CLI, you probably want to make a note of the service EPG. You'll see in the default graph, you have function node. Function node is the firewall function. If you do a multi-node PBR, you will see multiple function node. And the function node, will be deployed with a class ID, which represents the service EPG. This is the service EPG on which we're going to learn the MAC address of the firewall. If we jump to any leaf where a firewall is connected, that VLAN should be deployed on the port where the firewall is. We should find back the service EPG, and it has a specific flag, ep-service-enabled. This is a very specific EPG. It's automatically deployed. We don't learn IP from it. This is an EPG deployed with a specific parameter. Now your zoning rule check, that's similar to what we already saw. Back for east-west traffic in both directions, you'll end up to have four rules. The first rule is the rule from your consumer web to your app provider, and the action is redirect. Your second rule is the permit, where the source is the service EPG, your destination is your destination, and it's a permit. And rule three and four are the opposite rule. So you typically should have four rules. Here I'm having the same redirect number. Any idea why it would be the same redirect number? Any scenario where we would see two different redirects in the same contract? Sorry? Service chaining. Service chaining will be more complex. You'll have more rules. But the redirects identify the firewall function to which you redirect, the logical interface. If you do two arms, you're going to have different redirects. If you do one arm, you'll have the same, because wherever you go left to right or right to left, you redirect to the same interface because it is one arm. If you have two arms, you'll have different information. If we look at our redirect info, now we see the only difference to our previous example is that we have two possible destinations. And for each destination, we'll have rewrite info corresponding to VLAN and MAC address. So that's cool, but we still don't know if it's going to go to firewall pair one or firewall pair two. Would be cool to find out. There's two news on that. The good news is that we can find out. The bad news is that it's strictly impossible to remember the CLI to find out. Even me, I can't remember it. Each time I need to check it, I need to open my own slider. But you can do it. So it's like a show ether channel hash. You can do the same. So you can specify your source IP, your destination IP in this five-line CLI, your protocol, and it's going to tell you which hash and where it's redirect. So that's a good troubleshooting slide to keep. Not too long. I learned by heart. I would not... Yes, sir. Question. You said that we are doing the hashing based on source and destination. Would it happen that it's sent to the wrong firewall? So the question is about what happens if we do S NAT, D NAT, somewhere like that. Typically, the example where we mostly see where we introduce NAT would be a load balancer. It could be firewall, but it's mostly load balancer. If you do NAT, that probably means that your destination at some point is not the target server or the source server, but something sitting... So what we do typically for that is unidirectional PBR. So you have the capability in a service graph to redirect in one direction and not in the other. So for a load balancer, if you go from a consumer to a VIP, you will not have PBR in that direction because you want to reach the VIP. The VIP will send it to the destination, but the return traffic, we might redirect, for example. So there are multiple options to do it. But if you have NAT, typically, depending where you do NAT, you will not have PBR on both legs. Another tool that can be useful, you might have seen it in other presentations, is F3R. I leave it for reference. That's another script running on the APIC which will trace the packet. However, it relies on a constant flow. So you must have a constant flow flowing, even if it fails. And then it's going to run ELAM wherever it can and will tell you what happens. Starting in 5.2.3, it even tells you what happened after the firewall. It's not a very fast tool, but that's nice to have because that does the job instead of you and will tell you, I saw the packet on that leaf when you see L3 packets in, redirect. All of that gives some nice utility. Now, we've seen... Yes, sir? If we have one, would the return traffic go back to the original packet? Trying to find the drawing is going to be easier. So let's say here, the question is that what if we have an endpoint on the left talking to an endpoint on the right, right? It's an hashing. So it could be that your endpoint on the left will go to the firewall on the right. It's purely hashing. But the hashing is symmetric. So you might go from endpoint 1 to firewall 2, then to destination. Back from destination, it will go to firewall 2 as well because we hash the same thing. Yeah, but that's the reason we need the hash to be symmetric. So to be sure that whether we go from endpoint 1 to endpoint 2 and endpoint 2 to endpoint 1, we give the same hashing and the hashing is used to determine the firewall. So we ensure it. So that's why we call it symmetric. Yes. Could you go back one thing? Yes. In this case, if the ecosystem has to enforce, then it can happen that the backend is crossing the IPN three times. Even three? Yeah. So if you take the worst-case scenario with two endpoints on the left hashed to a firewall on the right, I think you can build a four-time IPN. So typically for multipod, it's not great. It's usually not much an issue because if you have east-west anyway, well, you'll know you call the IPN. But that's the price to pay. There is no way to fix it. But you're right. It can go multiple times through the IPN if you are a bit unlucky. Sorry? Oh yeah, you can. When I say pair, it can be three, four, five, ten appliance. I just simplify. Having the same pair across one. You can. So you can have the active on the left, the standby on the right. That's absolutely possible. That's also a popular deployment, but then only the active is visible. So you still will go to the IPN in some scenario. Let me carry on. So if we have other questions in that topic, I'm really happy to answer them after, but still need to cover the overuse case. So... So we've talked about multi-site. We'll first cover multi-site PBR, which was the only thing you could do up to last month. So multi-site, you would think, okay, it's the same drawing. I have a fabric on the left, fabric on the right. A lot of difference. Because it's a different EPIC cluster, and you have NDOs that manage it. So we have a couple of challenges. First is that typically, the EPIC in site one has no visibility to the firewall in site two. So there is no possibility to redirect to a remote firewall. Which is annoying. Because what's going to happen? I'm on the left, I'm redirecting in site one, so I'm going to redirect to the only thing I see, which is my local firewall. My local firewall will permit it, will send it to the destination. But the return traffic will do what it does the best, redirect to its local firewall. So symmetry is broken. So if we don't make some specific implementation in this scenario, we would have a fair amount, pretty much any flow which cross-site and needs to go to PBR, that would be asymmetric. So we took some implementation decisions. We can debate whether it was good idea or bad idea, but that's the way we implemented it. So to ensure symmetry, there is two things. We've decided that the consumer leaf, so the leaf where the consumer endpoint is, will never apply the redirect, even if it's capable to do so. So the redirect will always be applied on the leaf where the provider endpoint is located. Not necessarily the destination, the provider endpoint. So if your contract is provided by EPG2, that's where the endpoint in EPG2 sits that we will apply the redirect. Never on the other one. So that's ensuring symmetry. Because just based on that drawing, I can tell you that all my traffic will always be redirected on site 2, never on site 1 for that flow. No need for hashing. Because that's the way we implemented it. So even if Leaf101 is capable to do the job, it's not going to do it. So it's going to make a specific permit that will say, OK, I can apply a contract. I'm not going to do it. I'm going to be gentle. I'm going to leave the remote site to do it. We have an issue in the opposite direction. So we're coming back from the provider. We are in site 2. So if I'm the provider, I need to redirect there. I cannot send it to the ISN before redirect. So for that, we need to know for sure what is the PC tag of your source on the left. So even if the endpoint is unknown, we must have that information. The only way, at least up to come, we come up with a better idea was to configure the subnet of the consumer under the EPG and not under the bridge domain. By attaching a subnet to an EPG instead of attaching it to a bridge domain, we can uniquely map a subnet to a PC tag so we can always derivate the PC tag. So that's the config requirement with multisite PBR. Is that in NDO, when you configure your consumer EPG on the left, it must have the gateway IP there. For the provider, you can leave it on the bridge domain. If you fail to do so, I think in recent NDO, it will not be allowed if you don't do it properly. In older one, you could do it, but you're going to get some asymmetric flow. Not all, it's going to be a matter of luck, but you're going to get some asymmetric flow. So that's really the key point to remember. Yes, sir. Do I have to remove it from the BD or can it stay on the BD? You can leave it on the BD. You can leave it on the BD and you do it on the EPG as well. And typically in the EPG, you mark it with no default SVI gateway so that you leave the unique BD. That's what most people do nowadays. A few years ago, we were making only on one place, but now you typically do it like that. Zoning rule was different. On the ingress leaf, so the leaf which should not redirect, you'll see this specific rule. This is very bizarre. It's two action. So you have a redirect and you see the redirect points to a single firewall. We don't see anymore a choice. It's the local one only. But we have comma redirect override. The comma redirect override is used anytime the destination IP is not local. So that means I'm not the provider leaf to just say I'm not going to redirect. I'm just going to permit like if it was a permit but without marking the policy set. So it's not a permit. It's just say it's like I don't know what to do. So I'm going to send it to the fabric. No policy set. And I'm going to let the destination leaf do the job. So as soon as you see redirect override, most likely that's a multi-site PBR implementation. So that's still pretty simple. So that's very nice. We can make multi-site PBR. But we have put some serious restriction. The serious restriction is that the provider of the contract identify where you apply the redirect. A lot of people like any to any PBR. Good reason for that. That's extremely simple to implement. That ensures everything goes to the firewall. It doesn't blow up your TCAM rule because it's only a couple of lines. So that's a very nice and good approach to make security in your SCI fabric. But if you make any to any, who's the provider? There's no identified provider. Everyone is provider and consumer. So you can't implement any to any PBR multi-site with that approach. That's the problem number one that we want to fix. The problem number two is this famous EPG subnet. This is cool if you do network centric. But if you do application centric, if you have multiple EPG under your bridge domain, that's really annoying. You can't do that because then all you need to split the subnet. There is a lot of complexity. So for anyone that deploys some application centric approach or multiple EPG under the same bridge domain, it's a serious issue to have EPG subnet instead of bridge domain subnet. So these are the two issues we want to fix. And that's what we introduced in 6.0.4. And you also need the latest NDO. So you need both an upgrade of the fabric and NDO. That was better in 6.0.3. It works in 6.0.3. I tested in August and 6.0.3 was working. But it's officially available in 6.0.4. So we need to fix those two challenges. So the first challenge we want to fix is the directionless contract. So any to any means there is no provider, no consumer. So are we going to fix it? In a typical engineering way, instead of deciding, we're not going to decide. We're going to decide, I don't know, I want to be symmetric, I don't want to choose one side, I'm going to send it to both. So in any to any PBR, the implementation must ensure that every traffic from an endpoint consumer to an endpoint provider cross both firewalls. So your flow will look like that. On the left, you're redirecting site 1. You go to the ASN to the destination leave, in my case, leave 301. It's redirected. It's redirected to site 2. And from site 2, it goes to the destination and vice versa. It's probably not a big issue. So the state will be created on both firewalls. But it's very important to be aware of that. So you eventually, for the dimensioning of your firewall, so in any to any PBR, every firewall should receive the traffic. It's not a duplicate of the traffic. It's the same packet that goes in sequence to one and the other. So that's how we fix it. Now let's look how it works. I'll assume, and I know it's not always true, but I'll come back to it in a moment, that my leave 101 is capable to apply the policy. So my leave 101 knows the destination endpoint. So it can apply the redirect. So it's going to redirect it to its local firewall. Local firewall will permit it, create the state, and we'll send it back to the fabric. Now we need to do something a bit specific. We can't just route it to the destination telling I already applied the policy. We can't just make a permit because then it would go to the destination and it will bypass firewall 2. And it also needs to firewall 2. So we use a specific marking in the VXLAN header to signal the destination site if you receive a packet coming from site 1 with this specific marking in the flag. I already redirected, but you still need to do it. So it's a way to signal through the data packet, through the VXLAN header, that you still need to make the redirect on the target site as well. So another thing I didn't mention in multi-site for people that have deployed, they probably know it. We always make a translation of VNIT and PCTAC. So it's not the subject for today, but it could be confusing if you look at the material on the slide because the number, the VNIT number and the PCTAC number are always different in site 1 and site 2 because this is APIC allocated, not NDO allocated. So there is a complex translation mechanism, but it is done for every multi-site packet. So we reach 301. So 301 will see the VXLAN packet come from site 1 and it has this weird marking. So I'm going to need to redirect again. So it goes to firewall site 2. After firewall site 2, it finally goes to the destination and there is now a permit rule which we permit directly to reach to the destination. So we get that flow exactly as depicted with my red arrow. So remember, need to go to both firewalls always and that's the expected flow. Yes, any questions? No, okay. On the zoning rule, not a lot different. So you'll have your redirect on step 1, which redirect to the local firewall. Again, we don't have visibility to the remote firewall. In step 2, you have a new rule, a new keyword, permit and dof. Permit and dof is this specific marking. It means it's... I already redirected, but I need to signal to the other side, it still needs to redirect. So it's a specific marking inside the packet with the flags in the VXLAN header. In step 3, you have two possible rules. You have the rule ID on the top, which comes from shadow.epg to any, which is just a permit, permit and dof. We will not use it. We will have a specific access list, which is outside of the zoning rule, which is used specifically if we got the packet with this specific marking to redirect to the firewall on site 2, 172.16.1.2. And note the translation. The scope, which represents the VNID, is different between site 1 and site 2 because I got it translated in the spine for my multi-site flow. And finally, when we come back from firewall site 2, then we hit the permit rule. So we fix partially the challenge. We fix partially the challenge because by not deciding to send it to a specific site, but by deciding to send it to both, we don't really care about who's provider consumer anymore. This is good. What about the app centric? What about the EPG subnet? In my previous slide, I assumed that my Leaf 101 is capable to do the redirect because it has the endpoint information. It has all information to do so. If you don't have this EPG subnet, it's not necessarily going to be true. The endpoint learning of information in SEI is data path driven. It's conversional learning. So if my endpoint 1 never talked to endpoint 2, most likely Leaf 101 will have no idea of where it is and will not be able to apply the policy. So we need to still fix that. So for that, we need free functionality. As usual, when I say something, never trust me completely because there is always an exception. So one of the things is that we're now going to ensure that we have cross-site PBR node visibility. So we will allow and introduce the capability for a leaf in SEI 2 to redirect to a firewall in site 1. So what I was telling earlier is actually not true anymore for that specific use case. We'll have a pretty complex tromboning path because if we don't know, we're going to go left, right, left, right. So which is similar to what we say, which in multipod is usually acceptable because it's geographically closed. In multisite, that might be more an issue. It might be in two different countries or 100 kilometers apart. So this tromboning flow will work, but the latency might be high. So we want to avoid it at all costs as much as possible. So we'll introduce the third thing, which is a forced learning of the endpoint to avoid this complex back-end forced traffic. So with those three things together, we achieve a decent any-to-any PBR. We vote the need of an EPG subnet, so with a full application centric capability. Let's have a quick look at how it's implemented. You'll need first to have the multisite PBR tracking. Multisite PBR tracking, so you need to make sure that your firewall in site 1 is visible in site 2 and vice versa, and only if it's alive. So it cannot be a config pushed across firewall. We need to advertise it if we can reach it. So, we use SLA monitoring. So you need to set up an SLA monitoring on your monitoring policy. So leaf 102 will track, can I reach the firewall? And then it will give that status to the spine. And the spine will send it through a control packet, like an SLA as well, distributed across site. So site 2 will have, and that's what I'm showing here, will have a visibility of the received track service on the other side. So here I'm on a site 2 spine, and I see track service multisite received, so the VIP of the firewall on site 1, and we see it's up. Because the SLAmon verifies that the status is up. Once it's there, and we're going to propagate it to the leaf in site 2, and every leaf in site 2 will have now a zoning rule which is site aware. So you'll have the possibility in your zoning rule, so in your contract check, to specify a site ID to eventually redirect to another site. So let's look at the flow for the first packet. I don't know what it is. So step 1, leaf 101 has no clue. It's never heard about endpoint 2, it doesn't know if it's local or remote, it doesn't have the PC tag. So it does the only thing it can do to send to the destination leaf, regular routing, no PBR there. Leaf 301 knows that there is a firewall active in site 1, because it got it through the SLAmon probes that travel across the fabric. It knows it's coming from site 1, and it sees it was not redirected here yet, because the flag has not been set. So it's going to check, do I have a redirect rule pertaining to site 1, and it's going to send it back to site 1 PBR device. So firewall. And that's what we see here. You can now specify a sozoning rule in site 2 leaf, specifying site ID 1, and you'll see an action which redirects to an IP which doesn't even exist on that site, but which represents the VIP of the first site. The redirect points to the VIP, the pun to learn, I'm coming to it in a minute. So keep that in mind, there is an extra action, pun to learn. After the firewall in site 1, that's exactly what I explained before. We got it through firewall in site 1, so we're going to signal site 1 did the job, redirected done. Going back to leaf 301, which will see, okay, site 1 did the job, but I didn't do it yet, so it's going to go to firewall in site 2, and back from firewall in site 2 to the destination. So first it's extremely nice for a network engineer. Extremely annoying for me because it took me two hours to get all those arrows right. And for anyone that cares a bit about latency, if the site or distance, you don't really like my three arrows that goes from site 1 to site 2. So it works, no doubt, but it's not optimal. So we want to avoid it as much as possible. So we're going to do it for the first one or two packets, and then we'll use a forced learning, a control plane learning. So remember, I told you this point to learn. So that's in step two when it's getting the packet before the first redirect. So again, my leaf 1 doesn't know what to do. So he routed to the destination leaf, leaf 301. That one sees, okay, I can redirect to site 1 firewall, let me do it. But in addition, I have this point to learn. So I'm going to send it to the CPU. Let him do something clever for once. So a copy of the packet, will go to the CPU of that leaf, and that leaf will, by software, send the control plane packet back to the original leaf to force him to learn the destination IP of the server. So that the tromboning, which is not nice, happens during the initial learning. But as soon as one packet reaches site 2, we force the learning to site 1. So the next packet has the PC tag information for the destination, and don't need to go site 1 to back 1. You'll see that we do that. If you see an endpoint learn on the leaf 101, on the first leaf, and you'll see it's marked with control endpoint. So that's a control plane learn packet, not by data plane. And if you have a bit of experience in ACI, can someone knows what's the timeout of an endpoint in minutes or in seconds? Any idea? Five minutes. Five minutes is the timeout for a remote endpoint. Here I have 86,000 seconds. So those control endpoints are on purpose learned with a 24-hour timeout. So we don't like tromboning, so we avoid it for a long time. So every 24 hours, you potentially will have one or more of those packets going back. And that's kind of the conclusion of how the flow works. So, so far we've seen a simple PBR. And remember with PBR, you always rewrite the MAC address and make a layer 2 packet, and the spine must know the destination MAC address. It's your responsibility to be sure the MAC address is fine. For multipod east-west, you can have, as one gentleman suggested, use an active in pod 1, a standby in pod 2. But you can very well have active standby in both pod, and we have a symmetric PBR that ensures the symmetricity. For multi-site, it's more complex. So we've seen any-to-any is now possible only in 6.0.4. And before that, you have a strong restriction on the location of the consumer subnet. This summarizes the possible flag you'll see in zoning rule. So you probably feel like you forgot about it, but if you see any of those keywords, redirect override, permit end of, redirect, pun to learn, this is some of the specific use cases used by policy-based redirect. There are two great white papers. Maybe some of you already went to Minako's session, which probably talked partially about the same thing and even more features. So she wrote this white paper, which covers a lot of things. I don't think the any-to-any PBR is already there unless she added it lately. But the second document is on cisco.com as well and gives a pretty good detail about this trombone flow for any-to-any PBR, which can be a good reference as well. So I'll have the WebEx app. You can all join through the Cisco application. I'll be monitoring for the next couple, one, two weeks or something like that. So feel free to fire any question. And for the rest, I wish you a wonderful evening. And thank you very much to absorb such a complex topic at 6 p.m., the fourth day of Cisco Life. 