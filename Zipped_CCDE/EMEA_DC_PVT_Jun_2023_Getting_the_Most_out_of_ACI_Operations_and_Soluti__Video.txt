 Should I start or should we start? Okay, sounds good. All right. Hi, everyone. We're going to start with this session about the ACI operational best practices. The focus area of this session is about layer 2 integration and also a little bit of security. You already heard from Joyce asking something about endpoint security groups. So at the end of the presentation, we're going to build on top of what he presented earlier. So the agenda, no, wrong. So this is the agenda. We're going to start from the layer 2 networks integration with an example, which gives me the opportunity to explain to you also the latest enhancements that we introduced for layer 2 integration. So why and how to use virtual port channels? Should we run spanning tree or not? The answer of course is yes to run spanning tree on the external devices, but, and we're going to talk about what follows, using MCP which is something that I've seen, it's a bit misunderstood what MCP does and how it should be used. Then mitigating endpoint flapping that could be the result of a loop by using Roggen point control which is another feature that has been used or not used and there's a bit of confusion. And then we're going to talk about segmentation, how to segment the fabric with endpoint security groups, with or without flooding encapsulation and how to add a new element where we're going to add different features to make it very stable. So the example deployment assumes that you're trying to connect blade switches. So these are blade enclosures. So these two switches represent typically non-Sysco switches. Let's say HP for instance or Dell. And then these are the blade servers and the VMs running on them. And they are connected to ACI Leafs and this is the topology you'll know about with the ACI Leafs and Spines. Some assumptions on the blade server environment. So this is the blade enclosure and again you have here the blade switches with port channels and assumptions of what these switches are capable of they can run a spanning tree, the dot 1d version, potentially with rapid spanning tree for fast convergence. And because they're non-Sysco typically they don't run provenance spanning tree, right? So it's going to be either one instance or multiple instances of spanning tree but not one provenant. They typically, I mean port channels of course they can do, but sometimes they can also be clustered together. And then they have of course spanning tree features like a BPDU guard and these kind of things. So this is basic assumptions that are fairly representative of what's in the, in the market. Of course there could be other things that I'm not aware about, but this is all we need for this discussion. So first question is, how should we connect the blade switches to the fabric, to the ACI fabric? And so why it would be a good idea to use virtual port channels and how to use the virtual port channels? And I'm pretty positive that all of you would agree that it's a good idea to use virtual port channels, but let's try to, you know, look a bit more into the details. So if I'm not connecting blade switches with virtual port channels, what I'm going to do is I'm going to configure the ACI leaves to pass BPDUs because otherwise it would be a loop. And I would have to rely on the external blade switches spanning tree capabilities to keep in this case this port blocked, right? And it will be only this port block and not this one because remember that ACI doesn't run spanning tree so it's just a hub, it's just in that for spanning tree, it's just a hub, not in general. But so it's passing BPDUs and then to break the loop, this port will be to be blocking. So this approach without VPCs would rely on the external blade switches to do the spanning tree correctly and to block one port in this case, and of course more ports in a bigger topology. That's without VPCs. Now, if this spanning tree was disabled or there was an issue, you would have a nice loop which would not be nice, so you would experience a problem. Okay. So if you don't do any VPCs and if you don't configure VTropor channels, make sure you configure ACI to pass BPDUs so that this is the resulting topology. But the message of this presentation is actually that this is not the best possible implementation that you can, I mean the best possible design, okay? Now, if you use VTropor channels, this whole topology becomes nicer because all ports will become forwarding as you certainly know if you use ACI already, you would have a policy group type VPC for these two ports, 1, 1 on both leaves and then policy group type VPC, VPC2 for 1, 2 and 1, 2 and this would result in these two port channels between the blade switches and the leaves, and the resulting topology overall will be like this. So you would have a lot of port channels there. So intrinsically, this topology doesn't have any blocking ports, which is good. Okay. the first benefit of using VPCs. That's the first one. Now, the other advantage of using VPCs, especially if you're using also the link aggregation control protocol, is that the link aggregation control protocol is also going to do a verification on the links, and making sure that if they are joined together, it's because they should be joined together, not because of some static configuration. So, LACP is a good verification mechanism, and there's a nice feature in LACP, which is the ability to put a port into individual state. Individual state means that if you look at the state machine or the specifications for LACP, we see the I state is when the LACP partner, meaning the device that the leaf is talking with, is not sending any LACP packets on a link. So, the LACP side of ACI will put the port into individual state, meaning that port is not joined in any port channel. Is it good that the port is individual state? Where should it be instead part of a port channel? It depends. In the specific scenario that we're talking about, it's not a good thing because what we want is the ports to be port channel together, and if they're not, I don't want to introduce a forwarding topology that has a loop. So, it's a good thing to have this option called suspend individual, LACP suspend individual, which is an option that by the way is the default in the configuration of ACI in the LACP policy, and what that option does is ensuring that if you meant these ports to be a port channel, and if your blade switches are not configured for port channeling, then these ports are going to be brought down instead of being put into forwarding mode, and that can save outages. It can save outages, because without that option, if you disable spanning tree, or if you didn't configure the BPUs forwarding on the leaves, and if you didn't configure the port channeling, and you attach to the ACI fabric, and if you didn't have LACP suspend individual, then you could have a loop potentially. So, LACP combined with the policy group type BPC, also protects from this kind of misconfigurations. So, it's a good thing. Another benefit of VPC is traffic distribution. So, all of you certainly know about the port channel hashing capabilities. So, you can hash multiple fields of the traffic and distribute across multiple links. What probably is less obvious is that in case of VPCs, if you have only one link per leaf like this, it's not necessarily obvious that you also get low distribution for this topology. Okay. And this is not because of the port channel hashing per se, because if you think about it- Excuse me? Yeah. Can you mute yourself? Nope. Unless you have a- Okay. So, where am I? Sorry. Let's go back. Okay. So, what I was saying is that what's not obvious here is that how the port channeling hashing is helping. Well, here is not port channeling, but it's actually the VXLAN hashing that is helping in the north to south direction, but it's rather the ECMP load balancing, so the routing load balancing. And then the question will be how can that be because it's maybe later to traffic. Well, that is because in the VXLAN headers, and if you look at the specifications, it's possible to use the source UDP port to encode the hash of inner frames. And that's how VXLAN specifications, enable the load balancing distribution for encapsulated traffic. Okay. So, putting it all together, you have frames from the servers. Basically, the ACI leaf is hashing these original frames. It's encoding the result into the source UDP port. It's putting that into the VXLAN header, and then you have ECMP load balancing on the VXLAN headers, and that distributes traffic across the multiple layer three, three paths. And the end result is that you also get traffic load distribution from these leaves of VPC2 to the server. So, if we don't look at all the details, the summary is that with VPCs, you also get load distribution, not just for the server to ACI leaf traffic, but also in the reverse direction from the ACI fabric to the server. So, these are the multiple benefits of using VH report channels. So, more looping around, loop prevention capabilities, use of all the links for both directions of the traffic. How about spanning tree? So, should we run spanning tree on the external blade switches or not? Well, let's think about it. I mean, yes, the answer is yes, we should never disable spanning tree on the external switches because it's one loop protection mechanism. But let's think about it a bit more. So, the topology that we put in place with VPCs, which is this one, is equivalent to a bridge domain that I call here web, the bridge domain in the tenant configuration, with a lot of layer two links going to these individual switches. You could have more than one bridge domain, of course. You could also divide different bridge domains if you want. But by and large, if you have VMware or any virtualization technology, and you need to have the same VLAN available on all the blades, you're gonna have a large bridge domain with a lot of blade switches laid to external networks connected to the fabric. So, this is representative of a classic topology that you may have in your network. So, layer two domain with a bunch of blade switches connected to it. Okay, are there any looped paths? No, there aren't. Could there be? Yes, there could be. What could you do wrong? I mean, what could somebody do wrong to make it, bad to add a loop? Well, you could connect two blade switches to each other directly. You could also misconfigure the port channel, but as we talked about it in a few slides ago, this is solved by LACP. You could also connect blade switches of different enclosures together, and this also would introduce a loop. So, potentially, there is still a possibility to mess up this topology. So, is it, a good idea to run spanning tree? The answer is yes, but it's also a good idea to minimize the scope of spanning tree, and that's something that we're gonna talk about now. As you probably remember from past ACI sessions, ACI is able to forward BPDUs. So, here you have your blade switches running spanning tree. You can configure an EPG with the native VLAN. It's also easily accessible by using the visualizer. that is matching, that is allowing the forwarding the BPDUs and that makes sure that ACI will just forward these BPDUs from the root switch, which will be one of your blades, to all the other switches, which will point to this blade as the root. So if you have, let's say, 100 blade switches, one of them in one rack would be the root and all the other ones would be sending, I mean, would be considering the device as the root switch. So this is fine. I mean, it's a loop-free topology with spanning tree running on top of it. But there are a couple things to consider. First one is that from, you know that these switches are running also rapid spanning tree, .1w. But here, the EPG and the bridge domain are operating as a hub for the BPDUs, meaning that the BPDUs sent by this device are seen by all the other devices, okay? So the port type from a spanning tree perspective that you need to configure on these blade switches is port type shared. And that's actually documented. So if you go to the documentation, you'll find out. And that means the convergence time, should there be any change in topology, is 30 seconds, which is not very fast. That's one consideration. The other one is if you have a topology change notification, okay, in this topology, a TCN in principle, should not be at 30 seconds. happen because everything is forwarding anyway. There's nothing really blocking. But should there be one, the topology chain notification will be felt across the entire fabric. So two considerations. Spanning tree protocol is not the best one in the world. So you get the 30 seconds convergence time. And second, TCNs, if they happen, they are quite disruptive. It's not a good idea to have them. So is it good to run spanning tree? Yes, but here we're building a large, potentially a large spanning tree topology if you have a lot of lay switches, let's say hundreds of them, all of them part of the same lay-to domain. So it's a good idea to keep spanning tree enabled. But how can we minimize the scope of spanning tree to make it more constrained? So if these were Cisco switches, one possible approach would be to scope VLANs because you run provenance spanning tree. So if you scope VLANs on a pair of switches, then, of course, you would have a lot of small spanning tree domains. But with classic non-Sysco switches, like the Blaze switches, then you cannot do that, right? So one approach is to focus on the failure scenarios rather than trying to merge all the spanning trees together. So let's focus on the scenarios that could introduce a loop. So the first one is this one where instead of a channel, you have individual links. But as we mentioned before, this is solved by using LACP suspend individual. So spanning tree is not really doing much in this scenario. How about two Blaze switches connected to each other? Well, in principle, if you put spanning tree BPDU guard on ports that are not meant to be used for network connectivity, well, then when these two ports are connected, then spanning tree would disable the port. So spanning tree BPDU guard would definitely help in this case. And similarly, you also have spanning tree BPDU guard on ACI, the fabric access policies. So if somebody is connecting a port, that is not meant for Blaze switches connectivity, you can also configure a BPDU guard in order to disable the port if that is connected to a Blaze switch. So all in all, what we are saying is keep spanning tree running on the blades, but for this kind of topology with a lot of later devices all interconnected in the same topology, maybe it's a good idea to not enable BPDU forward. So we can do that by putting a BPDU guard on the fabric and instead to harden the topology by putting all these different configuration checks, meaning BPDU guard, and then using, of course, BPDU port channels. So the message here is let's not merge all the spanning tree together if it's a large topology. Let's keep spanning tree running on the blade enclosures. Let's use VPCs, of course, so intrinsically there's no loop. And this way, if there's a misconfiguration, the loop should not occur. Okay, so that's the message. But then this example, there are examples where instead you just have two modular switches and that's all your related to network that connects to maybe a fabric path network. Well, in that case, it may instead make sense to pass spanning trees. But in this case, in this example, it's not necessarily a good idea. So that's a summary of the later recommendations for this kind of scenario. Now, we're gonna add a few more elements to it that are very specific to ACI. One is the miscabling protocol, MCP, where we put also a few enhancements that you may be interested in using. And so let's take a look at another topology, and let's think about what MCP could do. So if you're using ACI already, you know about the miscabling protocol. And so here I'm showing you a switch, actually two switches, connected with a VPCs to the leaves. And then we're also adding this link, which I call link one or link two. And the first question is whether this additional link would be disabled or not by MCP. Okay, is MCP gonna consider this a miscabling configuration? Is it a loop? Okay, so that's a question. And of course, it depends. Okay, it depends on which VLANs are present on those links. Okay, because if there's no, well, if there's no EPG, assuming that this port is not connected to an EPG, then there's no loop, of course. Assuming that both of the PC and the link, individual link, are on EPG one, which are on BD one, then it could be a miscabling. So it could be a loop. By default, if you cannot clear VLAN one, or the native VLAN on the external blade switch, it is indeed considered a loop, okay? Because MCP will see the MCP frame going out on the VPC and coming back on this link, so it will disable this port. So, and that's right to do so. Now, MCP has a priority mechanism, so the virtual port channel wins over the port, over the individual link, and so MCP is indeed doing the right thing to disable this link and protecting the fabric from a loop. But how about the second switch? If on the second switch I can clear the trunk from the native VLAN, which Cisco switches can do, I don't know about the other vendor switches, then MCP here is not doing anything because there's no MCP frame coming back. And then this could be the right thing to do, because intrinsically there's no loop if the native VLAN is not trunked. But if I was to trunk the same VLANs on link two as they are on the VPC two, then it could be a loop, and in this case, MCP, the default MCP, would not detect the loop. And that's why we also have per-VLAN MCP. So, is the poll disabled or not? It depends. Is MCP useful? Yes, it is. I mean, at least the default MCP should always be used. And it's already protecting from one scenario. And it could also protect from this additional scenario if we use per-VLAN MCP. So what is MCP? Why, okay. This is a summary of what MCP is, which probably you already know. So it's sending these special frames to detect loops, mostly miscabling problems. And the default is to send MCP frames on the native VLAN of a trunk port. It's only sending them if there's an EPG connect configured, because otherwise it doesn't make sense to verify the link. And it can also be configured to verify the link on multiple VLANs. MCP is configured at two levels. There's a global level, and there's a per-port level. Both of them must be enabled for it to work. The port level configuration by default is enabled. And so if you enable it globally, then you get it running on all the ports automatically. Now, there's another option, the one I mentioned, which is the per-VLAN MCP, which is also something you can enable globally. And as of 6.0.2, it's also something you can enable per port. And that's very, very important. Because up until 6.0.2, if you wanted to verify all the VLANs in a link, you needed to do it on the entire fabric. And this is something that doesn't scale well, because just like spanning tree, if you need to send PPUs on every single VLAN on every single port, it's using quite a lot of CPU cycles. So now we have the ability to decide on which port you want to enable MCP per VLAN. And you can also decide how many VLANs you want to monitor. It used to be a maximum of 256. This has been now increased to 2,000. So there are more options per port that became available, and they should be used wisely, meaning especially for per-VLAN MCP. So per-VLAN MCP, it's not spanning tree. It's verifying all the VLANs or up to 256 VLANs in a link, or up to 2,000 VLANs in a link, depending on the software version. And if there's a loop detected, the port is brought down. It's not just the VLAN. It's the entire port. And so this is okay. It's a good thing to do if you have redundant topology like this one. Because if you have... Imagine you have your fabric path existing 5K, 7K network connected to the ACI fabric, and you run spanning tree there, but then spanning tree stops working. Then MCP would kick in, and it would disable one of the two ports and keep the other one up. So MCP in this case is doing something very useful, because it's protecting the port, you from the loop and it's also keeping one fabric active, one path active and it's good that it's verifying all the VLANs because then if there's a loop on any mean and then it's gonna help with that. If you're connecting the external A2 network with a double sided VPC like here, MCP here, the usefulness is questionable because MCP is gonna disable the entire port channel and so if you disable the entire port channel, you're effectively cutting out the entire A2 network. So yes, you stopped the loop but you also disconnected the existing A2 network from the fabric so it's not a good idea. So per VLAN MCP, yes, it's useful but it should also be used in a meaningful way but in any case, because of the feedback that we received from many of our customers, we increased the scalability of MCP so as I mentioned, now it's possible to monitor up to 2,000 VLAN on a port and we also increased the scale to 12,000 P comma Vs, port times VLANs and if the scale is exceeded, then ACI raises a fault if this fault you see here. And you can also monitor the MCP scale utilization by using this command, show MCP summary, which you may already use but we now have these additional options which give you a quick, quick overview of the utilization of resources so that you can stay within the scalability limits. So summary, it's a good idea to use MCP on all the ports. Whether to use per VLAN MCP, it depends. But just for you to know, we increased the scalability limits to make it more friendly to use it more flexible. Okay, now we have talked about how to reduce the chance for loops but you know, there are still scenarios where a loop or a flapping endpoint could show up. And that's where we have this feature which protects the fabric but it also, by doing so, it also limits the scope, the impact of a potential loop. So let's imagine that you have an external topology that is creating a loop, then you have a frame that is continuously moving between these two leaf ports on this bridge domain, so this is a bad thing of course, you know, it's gonna keep, you know, it's gonna create a lot of traffic and then you see the same MAC moving all the time and so there is a feature called Rogaine Point Control which has been there for a long time and it needs to be enabled globally and with Rogaine Point Control, this feature monitors how many times this MAC or IP is moving between ports and if it's more than a certain frequency, then basically it disables learning for that specific MAC or IP and it's quarantined for a configurable amount of time and then when the timer expires, then the endpoint is cleared from the forwarding table so it is then learned again. So it's a good feature, the frequency should, ideally you should not change it, it's six moves within 30 seconds above that number Rogaine Point Control kicks in. And then one of the main drawbacks of this feature is that the whole timer, meaning the time during which the MAC or IP cannot be updated anymore, was 30 minutes and many customers consider that too much because it was creating too long potential outages when there was a loop or some misconfiguration or something going on and so we reduced it to five minutes so now it's possible to, actually in my opinion, you should lower the whole timer to five minutes. When the loop or when the flapping happens, this fault is raised, that's a way to find out what's going on. The feature also tells you which MAC is causing the issue and where it is so you get a lot of information about which specific device is having problems. So if we apply this to a layer two scenario like the one we saw before, we have again potentially a loop, maybe we disable Spanning Tree by mistake, now if Rogaine Point Control is in the loop, we can disable Spanning Tree by mistake. So we have a loop that's in place and if everything else, for some reasons, didn't work, the problem is felt on this bridge domain on BD1, it's not felt on other bridge domains and that's because Rogaine Point Control kicks in and quarantines all the MAC, I mean not all the MAC addresses that are moving as a result of this loop. And so yes, for those devices you have interruption of connectivity but also your other bridge domains are not affected. So it is a good idea to enable Rogaine Point Control to enable Spanning Tree, and you can do that with Rogaine Point Control globally, but it's also a good idea to avoid too much downtime for temporary loops and the way you do it is by changing the old timer, the old interval from 1800 seconds to five minutes, that's the way to do it. The other point is that when you're making configuration changes, like imagine that you have operations that is introducing a new blade switch in your, in your fabric, your, in your RCI fabric, and you know that there's a chance of potentially introducing loops, it's a good idea to keep an eye on this fault and if you see it happening, then, and if you know that the issue has been resolved, because maybe it's a temporary issue, then you can clear the Rogaine Point, the quarantine endpoints manually, so that connectivity is restored. The assumption here is that if you know that this was, you know, expected because it's a temporary problem, it's fixed, then you can restore connectivity in less than five minutes by using either the UI or by creating a script that is just going ahead and clearing Rogaine Points. Having said all of this, there are scenarios where there are glitches that are detected by Rogaine Point Control, and they are temporary in nature, and one of them is, for instance, failover of firewalls. Certain firewalls, when they failover, they have, for some transient amount of time, they're using the same MAC address from maybe both devices, and that's not a theoretical scenario, it's a real one, we've seen it in multiple customers environments, and so, when you have these kind of scenarios, if Rogaine Point Control is present, the MAC address of the firewall will be quarantined for five minutes or 30 minutes, and if your firewall is in the path, then you have an outage because the MAC is quarantined. So, to avoid that scenario, it's possible to exclude specific MAC addresses from Rogaine Point Control, and that's something you can do from this release, from 5.2.3. So you can go to the bridge domain, and you can exclude the MAC addresses that are not, that you think, that you're not going to use. So a break, and it does 17.1 figures, or 10, it's the answer. accurate way to do that 오러, but at this point, those 15 open-endślę are allowed to flap during a certain amount of time. If the problem, TED is caused by IP addresses flapping and not by MAC address flapping , then the solution is to disable IP data data plane learning instead of excluding the MAC. And so that's basically another configuration and this you can do on the bridge domain itself. Okay, there's an option on the bridge domain to disable IP data plane learning. Okay, so two different configurations. One is if the problem is the MAC flapping and the other is if the problem is the IP flapping. Okay, now one key point that I would like to mention that we addressed in the latest releases is the fact that the flapping can happen not just on the bridge domain, it can also happen on the layer three out. Because when you configure layer three out SVI for connectivity to a firewall for instance, that SVI is effectively a bridge domain and if the MAC is flapping too much on that layer three out SVI, then Rog and point control also kicks in and it quarantines the MAC and then you get again some disruption. So to address this in 6.0.3, we're allowing you to exclude the MAC address not just from the bridge domain but also from the layer three out. And furthermore, in 6.0.3, we are adding a few usability enhancements which are very useful. One is the ability to define MAC addresses that should be excluded across the fabric from Rog and point. So if you have the same firewall vendor on a bunch of different bridge domains, instead of configuring this in every single bridge domain, you can enter it globally and then it's excluded globally. And also if you want to exclude all the MAC addresses from Rog and point control in a given bridge domain, you can also do that by using a star in configuration in the bridge domain itself. So this makes life much easier. So quick summary is if you're having firewalls, so quick summary, first enable Rog and point control, two. If you have firewalls in your network that are causing, I mean when they failover, they may have active active MAC addresses and so on, then you should exclude the MAC address from Rog and point control detection. Starting from 6.0.3, you can do it globally. So you just type that MAC address once and you're done. And if you're using previous races, you need to do it on the bridge domains where the firewall is located. Okay. So before you all fall asleep, questions? No? How many of you have had experience with MCP, using MCP? Okay. Is it useful? I mean what are we doing to enhance it? Is it okay? Is it meeting your needs? Yeah. We're increasing the limits, yes. 12,000 port. 12,000 port. Sorry? Scale was a problem. Scale was a problem. Yeah. Okay. How about Rog and point control? How many people are used it, have used it? Yeah. Okay. Is this addressing your needs or something left to fix? Okay. Okay. Okay. All right. So moving to the next topic, let's add the segmentation to the fabric. And you're, you certainly know about the joining of ACI, but let me, allow me to take a little bit, a step back and make a little bit of history. Prior to ACI, how was segmentation done? Well, you had VLANs and subnets. The flooding scope was the VLAN. Traffic was allowed by default. Segmentation, security filtering was done with ACLs. Okay. So that's prior to ACI. Then you could create isolation with prior VLANs. Probably a lot of you still use prior VLANs in the legacy networks. So you had the concept of community VLANs and concept of isolated VLANs. And you think of it, ACI EPGs are like community VLANs in a way. You can think of it that way. And then to allow traffic, you had again to use ACLs. Okay. And then you had the concept of the primary VLAN. So that's prior to ACI. Now, if we add ACI into the picture, it looks very similar in a way. You have EPGs, which you can think of more or less like community VLANs, especially with flooding encapsulation. Then you have the ability to create intra-EPG isolation, which is equivalent to an isolated VLAN in the end. And then you have contracts to allow communication between different EPGs. And you can control the flooding scope. By default, the flooding scope is the entire bridge domain. Okay. But then if you want to limit the flooding to just an individual EPG, then you need to use flooding encapsulation. Okay. So that's a way to see the evolution from legacy networks to ACI. Now, flooding encapsulation, let me go straight to this point, which is very specific. Flooding encapsulation has been widely adopted. I mean, we have many people using it. Why? Because to go to what is everybody now calls application-centric environments, one approach has been to use a single bridge domain, put all the solvenants under it, and segment it with endpoint groups. If you do so, then what happens is that everything is fine, but then flooding, of course, is the BD. And so if you have multi-destination frames of one EPG, they will be seen by other EPGs. And so with flooding encapsulation, you can scope the flooding to individual EPGs. This is fine, but if you're running with hundreds of EPGs under the same BD and hundreds of subnets under the same BD, which actually we have customers doing so, they may experience some scalability problems because the CPU has to replicate ARP requests on all the different EPGs because ACI has to do proxy ARP to scope the ARP itself. And that has some scalability challenges. So basically, if you have more than 1,500 port times VLANs, then ARP requests may not be resolved correctly, okay? So flooding encapsulation has been working great, but when you get to hundreds of subnets under the same EPG, same BD, and hundreds of EPGs under the same BD, you may have some, you may experience some scalability issues with flood inhibitors. So flooding encapsulation, okay. So let's talk about endpoint security groups because endpoint security groups enables you to migrate to application centric without this problem to start with, and in general, in a more elegant and flexible way, okay? So with endpoint security groups, you don't need to have a lot of EPGs under the same BD. You can have the classic one EPG per BD topology that the majority of ACI have. So you can have a lot of EPGs under the same BD that the majority of ACI users have. And then you can segment by creating endpoint security groups that are orthogonal, independent of the subnet. And by the way, tying to the previous slide, this doesn't suffer from the scalability concern that we mentioned before because even though sometimes you have to have, ACI has to do proxy ARP, there's no need for the ARP to be replicated across all the bridge domains because within each bridge domain, you'd only need one EPG, okay? So the control plane is also less stressed than it is with flooding encapsulation. So practical usage of endpoint security groups. So when you start putting security into the ACI fabric, the classic way to start is by using VZN to VZN contracts to allow any to any communication. The other classic way to do so is by using, you know, a permit all or a contract provided and consumed by all the EPGs, that's the other approach. Or the other approach yet is to use a preferred group and put all the EPGs in the same preferred group so they can talk freely and then start adding other EPGs that are not part of the preferred group and specific contracts within them. That's been the classic way to have more segmentation. But what if, you want to create multiple groups of EPGs instead of just one because preferred groups in ACI, there's one per VRF. So what if you want to divide your EPGs into multiple groups? How can we do that? Well, ESG is a way to do so. Okay, so can we do this in other ways? Yes, you could do it with a full mesh of contracts, yes. So, but then you have a lot of contracts to maintain. Or you could merge all the subnets in the same BD and then, you know, do what we were describing before with multiple EPGs in the same BD. But then again, it's not perfect either, especially if you have a lot of EPGs. But here we come, here ESG comes to the rescue. You could also create ESGs which are matching EPGs. So you can say that ESG A is matching EPG 1, 2, and 3. ESG B is matching EPG 4, 5, and 6. And basically, it's like, okay, so you can do this. It's like having two preferred groups where EPG 1, 2, and 3 can talk freely and EPG 4, 5, 6 can talk freely. And you can have, of course, contracts between the different ESGs. So you don't need to change the underlying VLAN configuration. You don't need to change the underlying IP addressing. So it's an easy way to create segmentation in your network starting from a network-centric deployment, okay? And it doesn't have the scalability concerns that I mentioned before for flood encapsulation. And that's a way that it's been widely used. And furthermore, if you then want to have more specific ESGs, like you want to say certain VMs belong to, like of a given application belong to a different ESG, you can also do that. You can add more specific selectors so that you have more specific IPs or more specific VMs that go into different ESGs. So you can easily create a segmentation like this and then you can have more specific ESGs later on. So these are the different ways to classify traffic. Some of them have been introduced from FIDO2. So there's the EPG selector we mentioned, the tag selector, which is basically the way to tag IPs and MAC addresses, and the IP selector. So you can easily segment your network and then you can easily create a segmentation. and then introduce more specific ESGs as you go by using the endpoint security groups. And we just published a nice white paper on this topic. You can find it there. And that white paper is actually using a very nice way to map the concept of application as it is understood by the application team to the segmentation as it is understood by the network team. And that's something that Joe was already mentioning. yesterday. Because oftentimes we run into this communication barrier between what application means to the application team versus what can be on the network side. And if we try to nail it down to individual applications running in individual server, which ports are opening to the other server, then you go into this crazy journey of trying to open individual ports and creating zillions of EPGs and it's gonna be a never ending project that is doomed to fail. There's a lot of things that we can do. There's a much more reasonable approach and that's described in that white paper, which consists in saying let's take all the servers that make, for instance, purchasing application or a ticketing application and let's put them all into the same ESG so you don't have to go crazy opening individual tiny ports and then use the contracts in ACI to allow communication between bigger environments, bigger chunks of applications. And there's a very good example there because the white paper has been developed. It's been developed on a real example, so it's very easy to follow and it's a very practical approach to understand how to segment your network, how to introduce application centric with ESGs. Before I move forward, questions on ESGs? Are you guys using already ESGs? Somebody started, yes, okay. Only one, okay. Too new or it's not too new anymore. Ah, the migration issue. . There is some migration problem between the EPG approach and the ESG because effectively, you cannot, you know, have contract between the EPG and ESG. Let me address that one. I mean, if you didn't know, maybe you didn't, but anyway. You can, if you have a contract between EPGs and then if you match the EPG from an ESG, the ESG is able to talk to the EPG via the contract. What you cannot do at that point is to add more contracts to the EPG that is matched by the ESG. But if the ESG is matching the existing EPG, you're not creating any disruption. You know, the traffic flow will still be allowed that was allowed before. And then you can start migrating the contract to the ESG. So we thought about the EPG selector, not just for what I described, but also for migration. That's the perfect scenario, but the application can be really tricky. Okay. Okay, other feedback on ESGs? . Sorry, here there was a... Okay, sorry about it. So I wanted to ask you about contract logging. Yeah. You can now, during the CSG migrations, it's very useful to enable logging on your contracts. I just wanted to ask you what are the best practices around contract logging? Should it always be on just for troubleshooting purposes? Well, the Denial plus Log is on, right? So the Denial plus Log, you can check the, you know, In the operational view, you can go and see the flow aggregated summary or the per-packet summary. If you talk about permit plus log, you need to add it to the contract, but there's nothing different with ESG versus EPGs when it comes to contract logging. So, it's the same story. It's rate limited, so of course, you cannot expect to see every single packet log, but it gives you an idea of what traffic is hitting. Then of course, for the counters, as you know, the counters are wrapping around. But if your goal is to see, there is this contract parser command on the CLI of the leaf, where you can actually see what's being hit. It's not accurate. I mean, it's not meant to be accurate. It's meant to be for troubleshooting and figuring out if the traffic is hitting those contracts. So, you can use those with EPGs or with ESGs. Then the chance of a re-remitting CPU is limited because there's rate limiting in place. So, it's designed in a way that, sure, it's not 100 percent accurate, but it's meant to be for troubleshooting. You had a question maybe? If I have a network where I have preferred group, can I just simply place those EPGs into the ESG and switch out the preferred group? So, the question is, if you have preferred group, if you can put just those EPGs into the ESGs. High level, the answer is, you should be able to do so. We need to check the priorities because you know that preferred group is creating basically a permit, any with priority I think 17 or something like that. We need to check the exact priorities. The individual EPGs have priority seven just like the ESGs. So, basically, if you match the EPGs from ESGs, what that is doing is changing the PC tag, the class ID of the EPG and changing it to the ESG. Now, the ESG could also be part of the preferred group actually. So, one way to do that is, you can do that by using the ESG. To do it, we need to check, okay? But it's high level, the idea would be you could make the ESG part of the preferred group. You could match the EPG from the ESG, and then finally you can remove the ESG from the preferred group, and there could be a way to do this migration, okay? So, the two things are not incompatible because ESGs can be actually also part of a preferred group if you want. So, the trick is just to think about the exact sequence that minimizes the disruption. And then operations, when, let's say, I migrate to ESGs, then I can handle the L3 outs the same way as I'm doing them with now in the preferred group. So, they don't, if I don't need to have a contract between L3 out and EPGs like I don't have now, can I do the same thing in the L3? Yeah, so you're asking about L3 out. So, for ESG to L3 out, it's the same as EPG with L3 out, meaning you have to have a contract. Now, if you don't have a contract between the ESG and the L3 out, you can't do that. So, if you have a contract between the ESG and the L3 out, you could make them part of the preferred group, potentially, if you want. Mm-hm. Now, if your question was, can the L3 out be just like, so if, actually, I think what you're getting to is this one. You're asking, your real question, I think, is this one. Your real question is, if I don't use preferred groups, and if I create multiple ESGs as a replacement to preferred groups, can I make the L3 out part of ESG? The answer to that is no. Okay. Okay. So, right now, I have a network. That was your question. I have a network with one customer, where everything is covered in the preferred group. They didn't want to go into the contracts. Yeah. So, L3 outs, everything is the same in a preferred group. Yeah. So, if you do use ESG- So, the original idea was just create a single ESG. Yeah. And place everything in it. Could it work, or I have to go to the contracts anyhow? Based- For L3 out. Your answer to your question is, you need a contract. Okay. That's the real answer. We are also thinking about making the L3 external matchable as an EPG from the ESG. But it's just a discussion we're having. We haven't done that yet. So, for now, if you do follow the ESG approach, you still need a contract between the L3 out and ESG. Yeah. But we're thinking about changing that. Okay. Okay. In the interest of time, I'm sorry, there's one more question. One more question about support of the ESG in the Nexus Dashboard Insight and Nexus Dashboard. Okay. Well, your question is about Nexus Dashboard. So, if you're talking about NDO, that's being addressed, okay? So, it's been worked on. About Nexus Dashboard Insights understanding, I don't know if somebody else from Cisco in the room wants to address that question, NDI and ESGs. Yeah. It's the same with NDO. It is being addressed and most likely in the same time frame. Future. Future. Now, but it's a concrete future. It's not just like sci-fi future. Yes. Okay. Let me go to the very last topic. Just to, to conclude the presentation. The last topic is first-off security. I don't know how many of you have used or looked at first-off security with ACI. One person, two people? Okay. Okay. A few people. Okay. So, first-off security is a feature set. Of course, it exists in ACI just like in classic switches. It can address multiple securities, multiple attack scenarios. This example here is focusing on R poisoning. So, somebody could potentially compromise, claim to be another host like in this one, this host is claiming to be IP1. So, that traffic directed to host one goes to host two instead of host one. Or another scenario is that a host could pollute the R table of another host and receive traffic was destined to the gateway. So, first-off security addresses, these scenarios and more of course. How is it configured in ACI? It's configured in two places. The bridge domain where you select the type of feature you want to enable. Then it's configured in the DPG when you can create trust policies. So, you can say whether you want to trust the DHCP server there, whether you want to trust ARP or Neighbor Discovering Sun from a specific EPG. But, I say like for the scenarios I described before, probably what you care about the most is just enable one of these features like IP inspection. There's more than that, but for R poisoning attacks, IP inspection would be one of the features that you would care the most probably. What's new here compared to what it was before is the fact that in 602 we added support for the integration with VMM. So, with vSphere and so on. That wasn't there before. And so, before you could do first-stop security only for physical host. Now, starting from 602, the feature has been extended also to work with virtualized environments. Okay. So, since this feature is not being talked about for ages, and I'm talking about it now because we introduced the support for virtual machines. What is this feature doing? Is learning the endpoints not anymore by data plane learning when it's enabled, but by verifying that the endpoints is actually a legitimate one. And it's building a database of endpoints that are verified. So, it's a secure database. It's maintained by each leaf. And so, when there's a new device that claims to have the MAC of an existing device, then ACI is going to verify if the previously discovered device is still there or not. And if it is, then it will not allow this to be learned. Okay. So, that's the details of how this feature is working. Now, if you have that feature in place, then the attacks that was described before would not occur. They will be prevented. So, this is an actual what first-stop security does. And again, the novelty here is the fact that it's supported by virtual environments. Now, it comes with some scale limits, which is 2,000 endpoints and 1,000 bridge domains. So, the limits could be not applicable, not useful for everyone. But I just want you to know that we put some enhancements there. And it can be used, of course, also in conjunction with endpoint security groups. So, the question is, how can it be used with endpoint security groups? Well, you would enable this feature on the bridge domain and the EPG that is used to connect, you know, servers to the fabric. And it would not be enabled on the ESG per se. But the ESG portion works in an independent way from this portion. So, there's the verification that happens at the EPG and bridge domain level. And then there's traffic filtering, which happens at the ESG level. So, the two features are completely compatible, and they complement each other. Okay. So, that's all I have for this session. We ended up earlier. Yeah. Okay. Yeah. Okay. So, a couple of questions that we got in the Q&A. One is, any plans to include for endpoint rock control a syslog message and not blocking the endpoint? So, only informing. And the second question is, any plans to increase the number of... Let me see. The exceptions for Roggenpoint? Yes. So, answer to the first question is, we are discussing the first option, which would be like a learning mode, let's say that way, maybe. It had been brought up multiple times where Roggenpoint... Could we use Roggenpoint, actually monitor what Roggenpoint would block before actually enabling Roggenpoint? Nothing committed. We're still discussing that. About the second part. The exact scalability numbers are not yet published for 6.0.3, but we are increasing indeed the support for the exclusion list. So, there are two aspects to that. How many can we configure in the exception list? That's one thing. And the other thing is, how many are quote unquote supported? And what I mean by this is that since we're allowing the configuration globally in this release, in this... In this 6.0.3 release, then when you explicitly specify a MAC, that MAC could be on 100 bridge domains, which means that, yes, you enter one entry, but in fact, you're configuring the equivalent of hundreds of entries. So, there's going to be an increase on the number of entries you can enter, but then there's also a number of how many total entries that are a result of this configuration are going to be supported. And the number, it's a bit preliminary. I don't want to, you know, all disclaimers apply here, but the number the QA has validated so far is around 6,000, 5,000 total MAC addresses, BDs that are being, that can be excluded. So, yes, we're planning to increase that, yes. Thank you. Okay. So, if there are no more questions, thank you. And, yeah. Finished early. Thank you. Thank you.