 All right, perfect. So, hello, everybody. Good morning. Good afternoon. Good evening. Wherever you're located. Welcome to today's very special joint compute champs and DCN ninjas super session. We're titling it all about Cisco's vision and strategy for AI and the data center. So thank you so much for joining. Yeah, let's jump right in. We have a very concise agenda for you today and really great speakers lined up. So we have Brian showed up our senior director for Cisco global partner sales. Jeremy Foster, our senior and general manager for Cisco compute. And Lucas credit your Cisco fellow for data center, networking engineering as well as some panelists from the team who will be helping with Q and a. Spencer menard solutions engineer, the Monica, technical marketing. Okay. Engineering technical leader and James Terry sales specialist from the compute sales go to market. So thank you so much everybody for jumping in and supporting in today's session. On to the agenda, so Brian will be kicking us off with an introduction. For the session, then Jeremy and Lucas will cover what is on the, you know, on the track today for compute and in a plus the latest announcements that were communicated during Cisco live us some weeks ago. Followed by Lindsay and myself will be talking about our call to action from partner go to market. And finally, in the end, we'll have time for Q and a. so as our presenters are speaking, please feel free to send any questions you might have through the chat any comments and we'll make sure to answer them at the end. And we'll make sure to answer them at the end. So, yeah, thank you so much. And Brian, yeah, thanks Maria. And hello everyone. Good morning. Uh, maybe. Uh, I'm not sure if I'm going to be able to answer all of them. But I'm going to be able to answer them. So, yeah, thank you so much. And Brian. Some good afternoons and some good evenings, uh, hope, uh, hope everyone's doing fantastic. So, uh, I'm super excited as Maria highlighted that, uh, actually, I think this is the 1st time we, we have the champions, uh, and the ninja communities. Uh, together on 1 call, so I just want to thank all of you for helping us. Uh, and and really, uh, driving the, the message and the strategy into all of our key customers that you're engaging with. And, uh, and I can't, uh. really imagine all of the things that you're doing out there. And so now with that, we've got a pretty important topic today. And AI is hitting all of you. It's hitting all of us in ways that I think are bringing a tremendous amount of opportunity as we think about what we could be doing together, what we could be doing for our customers, and really excited that I'm here today with Jeremy and Lucas to talk to you about what's happening in the market and also what is our strategy? How are we trying to differentiate? And why is really now the time that we need you to really be leaning in? And we want to help. If you can go to the next one. Okay. So some of you might have seen this before, but so why is especially Gen AI becoming so interesting? And what you're seeing here, this is actually some work that OpenAI had done with some university research. And it's looking at the performance of GPT 3.5 compared to GPT 4. Now, as many of you know, GPT... So, you know, we've been talking about the performance of GPT 4.0, Omni, and obviously there's been a lot of other innovations in both private and open source models. But it's a little bit difficult to see at the bottom, but these are all SAT and AP high school class testing. And you look at the increases in green. That's all of your GPT 4.0. And so when you look at whether it's Claude Mistral, the OpenAI GPT work, it's really now starting to accelerate the ability for these models to really deliver a tremendous amount of really high impact, accurate reasoning across a lot of different subjects, a lot of different topics. And that's why really now is this. This is the time that we really have an opportunity to work together. And by the way, this is not just across LLMs, but there's now high impact, as many of you know, SLMs, smaller footprint models that we're seeing a lot of customers express a lot of interest in. And so, again, right now, it is just the right time that we can together be leaning in on these opportunities. Go to the next one. Brian, it's too bad they didn't have that when we were trying to take the SAT. Because I don't remember any of that kind of assistance. Well, that's why we're here. That's why we're about powering the future for students. So what are we really trying to do at a company level? And I think if you remember just one thing, it's connect and protect. And we're going to spend a lot of time talking about infrastructure. But I think something that you're going to be very excited and very curious about is what can we do together to help you on the services side to really accelerate the value of AI. So more to come on that. Next. And really from a journey standpoint, I just want you to memorialize that we now have a set of capabilities that we're ready to help you at any value. I think it's going to be a very, very challenging phase of where you are. And if it's at the model selection and rationalization, all the way through to deployment, tuning, whether it's RAG based and other motions. So really there's a lot of different pieces that we're going to want to spend a lot more time after this call to help you kind of see what those components are, the vision, and how we can help you execute. So, Brian, I think we're going to have a lot of fun. We go to the next one. And this is really a high level view, but you can kind of see all of the blue areas here are the areas that we're really have very strong coverage in. The green areas, as I mentioned, we obviously are doing quite a bit of work and putting some investments into some of the companies that are building foundational models. Jeremy is going to talk specifically about some of the partnerships that we're working on. And then we're going to talk about some of the other things that we have on the compute side. But, again, there's a lot of pieces that have come together over a period of time. And we're really excited to talk to you about all of these components. But, again, today's focus is really going to be around the infrastructure pieces. But, again, just kind of eyeball this to know that there's various pieces that we're going to be able to help you with. If you go to the next one. And some of you that have been following even some of the recent investments at Cisco Live, we announced a billion dollar investment fund. We've actually already been deploying quite a bit of money into companies. Some listed here. And, again, as I mentioned, Mistral and some of the others. That's obviously one large piece. We've done a fair amount of acquisitions. Probably the most prominent one being Splunk recently. And we're going to talk more about these partnerships. So, again, a lot of different pieces going on here. And if I show you this next slide, Murray, if you can go to the next one. We've done some work around looking at, hey, where's all the money going? And this is just kind of a visual. And, by the way, this slide. Is managed by a team inside Cisco. And I'll tell you, in recent times, even between November until now, it is very difficult to keep this thing updated. But you can see just the amount of money, the amount of, you know, you look at some of these some of the funding rounds that are going in. It's pretty amazing. The amount of money that's being invested. And obviously Cisco is playing a pretty big role as well in not only partnerships but also on the investment side. Go to the next one. And as I mentioned, the when we talk about wanting to accelerate AI together with you, I just want you to just memorialize services we know is probably the most important thing for us. And that's probably going to be the most critical piece. And that's from start to finish. Into deployment. Into renewals. From a core AI context. The other piece that I would just say is that AI for to help accelerate or even shorten the time that it takes for you to consult, for you to help drive customer decisions and so forth. There's different pieces there that we're going to want to circle back. There's a lot of different pieces that we're doing to just make it a lot more efficient for our own workforce, our own tooling, our own workflows and really trying to drive significant amount of productivity. Those are two different areas that I just want you to continue to memorialize because there's a lot of different pieces there that we're super excited to work with you. And then the last piece is really just to help you on and help really drive the acceleration of any deals that you have in the pipeline and making sure that you're able to drive those with a high level of gross profit and gross margin. And then just the last one here, Maria. Before I hand it off to Jeremy and Lucas. There's a lot of different things that we are rethinking as it relates to AI. And I think that journey will continue to go on as it relates to how we're innovating with silicon, liquid cooling. There's a lot of different pieces. And again, this is kind of a market and a customer segment to view, but we're playing really meaningful roles in all of these. And so some of these you see in kind of public announcements, blogs around, but more to really come across these vectors. Now, let me just pause here real quick. And introduce Jeremy and Lucas here. If I hand it off to you to kind of walk us through some of our recent announcements, the innovation and strategies. So, Jeremy, over to you. Absolutely. I think, first of all, thank you all for taking the time today to be here. We're running a couple of minutes behind, so I'm going to go through these announcements quickly. I know some of you may have seen some of these things, but important, I think, to talk about the data center space and where we're headed. Overall, whether it's compute side, data center networking side, we're working together these last few years like we haven't worked together before. And we're really, really excited about our portfolio, the things that are coming out. We'll talk more about that later. Those are really the results of additional investments that we've been making, specifically, at least in my case, for compute in regards to getting some of the new things out that I'll talk about here in a minute. And we plan to continue to invest in the data center space and accelerate. We'll talk more about that later. Those are really the results of additional investments that we've been making, specifically, at least in my case, for compute and accelerate our product deployments here to make a much better portfolio for you all to be able to take to market. And I think as we go around out this year and move into next year, at least from the fiscal perspective for Cisco, just very, very pleased with the progress we've made in terms of product development over these last few months. And you're just going to start to see these products rolling out right now and becoming orderable. And I think it's really important to just take a step back too and look at the fact that I've been in this data center space for a couple of months. space like many of you have been for a long time, longer than I probably want to say, but say north of 20 years. And I don't remember the data center itself ever being as important as it might be right now. So whether that's the AI pieces, that's the transition around everyone's favorite acquisitions that are taking place and hypervisors and every customer asking us what's their best move forward. There's a tremendous amount of infrastructure that's in play in the data center. And it's a great opportunity for all of us to be able to help our customers come to a really good outcome and honestly just really step up and be able to drive those outcomes with them. I think it's going to deliver a tremendous amount of dollars and great customer experiences. So those priorities for us is across the data center portfolio, clearly this VMware transition, which by the way, one of those outcomes is going to be just stick with VMware, but help customers consolidate down that data center to minimize that spend. But also what are we doing with Nutanix? What are we doing with Red Hat? And what are we doing with the rest of the ecosystem to give you all options to take to customers as well? Simplifying infrastructure for AI, we'll talk about that. That was certainly one of the big announcements at Cisco Live. Security in everything we do. Another big announcement at Cisco Live was HyperShield. That announcement happened a little bit before Cisco Live, but we also talked more about it at Cisco Live as well as the ecosystem. So that's a great opportunity for us to take a look at that. Integration of offload capabilities into UCS as well. So we're working with the security team, we're working with the data center networking team. You'll start to see compute be more of that fabric of the underlying applications at Cisco as well. And then we'll talk a lot about management consolidation in our talk track today as well, which is a key development priority for the next couple of years. Go to the next one, please. So the thing we're always focused on doing is giving everybody the best building blocks. I know from being in the field for so many years that it's so important when you have a mousetrap that is better than somebody else's mousetrap, that's a great way to make sales and delighted customers happen. And so when it's the data center networking portfolio, whether it's what we're doing with our optics portfolio, what we're doing with UCS, those individual building blocks, we have to focus on the data center networking portfolio. And so when it's the data center networking portfolio, we have to focus on continuing to make better and have a broader portfolio solutions for you all to take to market as well as our validated designs. So how do we bring these together into solutions for AI, for VMware, for any of the use cases like you've seen us doing for many years, but to continue to improve upon that. And if we go to the next one, we'll hit several of these in terms of these announcements for Cisco Live. On the hardware front, on the building side, we're going to be working with the data center networking portfolio to help you build these things. So the first one is the AMD blade. It's the first AMD blade. It's the first AMD blade that's orderable today. It'll start shipping here in another 90 days or so. So really excited to have a modular form factor AMD blade that can handle kind of the latest and greatest speeds and feeds from them in the portfolio as well as the rest of the rack mount servers that you would expect from Cisco to complement that AMD-based. Those are also orderable as well. X-series direct. I know people on this phone call may be familiar with what we used to have with UCS Mini in the previous chassis. X-Series Direct ties that together, plus additional intersite functionality to really bring together a solution for customers to leverage X-Series in a lot of different use cases when it's not attached to that scale-up fabric. So whether it's a DMZ small area use case inside of an existing data center, or whether it's a customer who needs a couple of servers, a couple of GPUs, and have some decent-sized infrastructure out at the edge, X-Series can be a great opportunity for that with X-Series Direct and a really cost-effective way of deploying and managing those workloads at scale. Lucas, any of the ones you want to mention here before we jump into… I mean, there's so much, Jeremy. I mean, I… I don't want to go back what you said and don't want to expose your age, but it was not called data center back then, right? And it was not called a server. It was probably more of a computer in a rack somewhere in a server room or something like that. I think we've been there in the same one, the shady cornice there, but thanks, Jeremy, for bringing that up. So Jeremy mentioned fabric. We in the data center talk about fabric so much, and it becomes kind of a tiring… kind of term, but, hey, we have new fabric announcements, as it always comes. So one thing we extensively did over the years is looking at operational models, and we figured there are too many of it within Cisco, so we started trying to get them close together, and we'll have a couple of more slides as we go through. But we announced Nexus One Fabric Experience. It's all around the experience, which we're looking at in combination with Nexus Dashboard 4.0 or the Unified Nexus Dashboard, as some people use it. Hyperfabric was a big thing, again, sorry for the fabric term there. And then, of course, how we're getting to the next speed levels when we look from a switching perspective, is it for AIML use cases or beyond them? Were we going all the way up into the 800 gigs or so? So these were some of the announcements we came out that Cisco Live and we continue to drive as we go through this second half of the year. And there is more to come. I just say as much. Jeremy? Yeah, absolutely. Well said. But we can maybe hit a couple more things on compute, and then we'll jump back over to Datastore Networking and go through some of our joint announcements. I wanted to remind everybody about fabric-based computing, back to that fabric term. I mean, this is really where we differentiate. It starts with that intersite experience represented by that monitor at the top there, and then our capabilities around helping customers deliver the automation for these policies, profiles, and templates, and ultimately be able to manage these environments. So whether it's VMware, Nutanix, it could be effectively a profile for something like Nutanix AI GPT in a box, right? Could be the new solutions that Red Hat's rolling out in this space. But we can make it very easy and cost-effective for customers to reduce the amount of physical footprint that they need to take up with compute, have better performance where we're about to talk about some of our new AMD performance testing and setting. We'll record benchmarks with those platforms as well. You'll see that hit some of the public channels here pretty soon. And just overall, greater flexibility and control to manage multiple workloads and to manage multiple hypervisors in the environments. I think this is a key differentiator for us, particularly when people are looking for alternatives. And I think we'll probably see a state that we haven't seen before in the data center, certainly around virtualization, where maybe customers will be leveraging multiple hypervisors depending on use cases. So being able to keep up with all of that is something we've been doing for a long time. And it's going to be something that helps us, I think, pick up new customers that have a good experience on UCS. Whether that's on a modular server, a rack mount server, it doesn't matter. Those capabilities are the same across any form factor. And then just to give you on the next slide a real quick view of what we mean by those use cases around X-Series. X-Series Direct ties into Intersight. Right. And manage things at scale, whether it's 500 servers in a data center or whether it's 500 locations with eight servers apiece. But as you can see here, this just gives you the ability to tie that fabric that we normally scale out on top of inside of the data center and put it into the back of an X-Series chassis and then even tie in some additional rack mount servers. So you can run up to 16 boxes off of this one X-Series Direct. You can manage it directly from Intersight. So you can scale off that SaaS-based management platform. And get the right balance of storage, compute, performance that you need at even those edge locations. So just wanted to put that in front of you as well before I think, Lucas, I'm going to hand it over to you to talk a little bit about some of the joint solutions. Sure. Absolutely. Thanks, Jeremy. A year ago, Cisco Live last year, we announced the Cisco Networking Cloud. And there was a big round it. What is it? When is it coming? And actually, I want to tell you that you all are probably in the midst of the Cisco Networking Cloud today. So the Cisco Networking Cloud, as it comes to the data center, is all about design principles, Cisco itself, the product we're building, and the ecosystem which it fits into. As part of the Cisco Networking Cloud, it's how you experience what we're doing. You see three attributes or three areas where we're trying to bring or where we actually started to bring the Cisco Networking Cloud. This is one category directly heading into the. estrogen and cybersecurity aspects. And when how do we propel the co-management cycle to the software? This includes Into Guys. This is another key part of our<|en|> TO maybe different attacks to image for advice. So let me show you one example, we will be focusing on this in a little bit, but when the question came into question with an image recommendations experts. Once we started youngsters were saying that they were starting to lose touch with encrypted information really from Open linked ganzuer because they couldn't close it, establish a network altogether within a cloud and not intendлиーweisen we want to do end-to-end secure networking. Of course, we want to attach the secure compute into that area as we latch it on or attach it on. But the networking cannot just be a packet forwarding. It has to be secured as Brian already managed before. We want to make sure the workloads we're having are secured and there is an opportunity for us. The other part is in regards of simplified operations and specific to simplified AI native operations. Everyone knows artificial intelligence infrastructure is a very high demanding infrastructure. It's a very data intensive infrastructure and such has requirements to data flows being undisruptively being forwarded across your network. I'm sure you heard of neural networks and how such GPU to GPU communication becomes super critical to be forwarded at all time and not dropped artificially. So we not just want to, build a network that is better for AI workloads, but we also give you the operational aspects for that. And as part of Cisco Networking Cloud, we embodied that part in our products, which I'm going to show in a second from now. Next to that, assurance. The digital experience assurance is very tightly coupled with what your operational experience is. Configuring something is one, understanding how a flow goes and then assuring that that flow is really going in that direction. This is kind of the combination of where ops and assurance comes together in the combination. Next slide, please. Click to the right. Thank you. There's a bit of lag. That's fantastic. I love it. So we don't want to have lags in the network, but sometimes it happens. It's how it is. Thanks. We're talking about the Cisco Networking Cloud. We're actually talking about the Cisco Networking Cloud in general. So we're talking about the combination of the Cisco Nexus One Fabric experience. So that One Fabric experience, the same experience across Nexus and, sorry, NXOS and ACI is one of the things we took to our heart. We heard a lot of customers saying, hey, you make it too complicated. You have too many variations. Can you give us a simpler and easier way to consume that? And the Cisco Nexus dashboard is going to be that single point of control and operations, which will allow us to do that. And we'll give you that One Fabric experience as a result. So it's not about converging, merging, or whatever else fabric types you can think of, because there will always be very specific and very specialized use cases for our customers, which we want to continue to maintain. But we try to give you a better experience, a more consistent experience when a network being deployed in an ACI infrastructure should look more like how a network you could deploy in a VXLan EVPN network. Or for other network types as you wish. So the Cisco Nexus dashboard will be more or less the front end of what we call the fabric federations, meaning the interconnecting and ability to deliver common use cases across our different networking fabric types, which the Cisco Networking Cloud is a common attribute, a common guideline, which we're having as we're building Torbit's Unified Nexus Dashboard. So we're trying to get that to work. Lucas, I may pause you for one sec. And open the floor. Anyone have questions, comments? Rita, I saw you may have unmuted. Not sure if it was a question or anything you'd like to add. Silence is golden. Continue on. Fantastic. Now you stole my transition. But that is fine. I'm so sorry. No, you're good. You're good, Lindsay. It's just all joking. Okay. No, we were talking about that Cisco One Fabric experience, and now Cisco brings Hyperfabric, which gives you another fabric experience. And the answer is no. So the Cisco Nexus Hyperfabric is built on the same base principles as the Cisco Networking Cloud or the One Fabric experience, so it comes together under that whole umbrella. What we focused on when we looked at Hyperfabric, or Cisco Nexus Hyperfabric, is bringing a different footprint layout, to our customers. They're customers that might not want to have an operational footprint from a tooling perspective. They're customers that might not care about a control point or such. So that's where we moved the operation and control pieces into the cloud with a software as a service model. So we're delivering the controller as a SaaS. You could call it the Merakification of the data center. As such, that control point is the only way to interact with the switching system. We built specific switches that are connecting to that SaaS controller model, which will give you that simplified fabric experience you also see on the Unified Nexus dashboard as part of Cisco Networking Cloud. In Cisco Nexus Hyperfabric, it's all about design, order, deploy, validate, monitor, upgrade, and collaborate. Somebody had to put so many things in there, so many words down there. It could have been just, you design it, you order it, you build it. It's all out of the SaaS offering. It's all based on the models we built for you. There's not much network engineering itself in there. You select the switches, you select the interconnectivity, you order that piece, you deploy it, your Nexus Hyperfabric will deliver it as such. Now, given the Nexus Hyperfabric is very focused on that cloud delivery model of a network, and became much more simplified as such, we decided to make this the going forward networking full stack for AI cluster. If you think of where the focus of an AI infrastructure is, or the AI operation team, it's around data science. It's not around operating a network. So we want to remove that aspect from it. Regardless of this, the in-time delivery of data between the GPU cluster, between the AI cluster is important. As such, we're expanding our monitoring capabilities, our configuration capabilities into the host for monitoring the NVIDIA GPU or DPU NIC part to make sure that whatever is configured on the workload side matches on what the networking delivers itself and give you an end-to-end operation. In addition to this... There's a quick one that came in. It was, does Hyperfabric include any Nexus devices, which I know the answer to, but I'm going to let you... Yes, at this point, Cisco Nexus Hyperfabric is focused on the Cisco 6000. So it is not a Nexus switch at this present time. We'll look on how that goes down the road. The potential is there, but even if we would add it, it would be a Greenfield only deployment. So reuse of hardware at that point. But yeah, that's a good point. Good question there. But just go back to the Hyperfabric for AI cluster. I want to make sure that we covered that entirely. So it's from the workload, the server, the GPU, the DPU, the storage, the networking as one, as one combined AI infrastructure for you, all based on Ethernet and IP. Next slide. So I spoke about the Hyperfabric for AI, which you see on the very left-hand side, the SaaS solution or public cloud managed solution for enterprise public sector commercial areas. And you see there, there is an FCS target, which goes slightly into the next calendar year. So I want to shift a little bit on what is shipping today and what are the solutions we're pushing today, we're pitching today, we're giving you today to, to achieve a similar or the same approach there. For one on the very right, we do for our hyperscaler friends, very specific solution, very customizable solution around Sonic and bring your own operational model that you see on the right-hand side with the Cisco 8K. Our business and data center networking, the focus where we have today is around Nexus 9000 with Nexus dashboard. We are doubling up in the switching power we have in there and the operational power when it comes to AI, ML, Ethernet fabrics, when it comes to what we are doing today, what we're shipping today with customers and deploying today with customers when it comes to AI and ML workloads. We have not just the 9K with some not NXOS on top of it, that will give you the necessary forwarding power, the speeds and the feeds, the feature, but also the Nexus dashboard, which helps you on the operational side. So you're becoming much more of a, network plus operation stack as such, which then if your customer has a demand going into one step further, we will go and deliver that as a full stack solution, as you see on the left-hand side of the hyperfabric. Next slide, please. So as you can see here, this is almost the accumulation of every data center technology in here when we look at what is needed for an AI, ML approach. As we start off, we have to switching power. We have the Nexus 9000 with the high density, 400 gig ports today. We're delivering them in a form factor of 32, 48, as well as 64 ports and the respective optics for that to build such a non-blocking spine leaf fabric, as you see in the middle. Now this non-blocking spine, leaf fabric, everyone can stitch it together from a cabling perspective, but there are certain attributes which need to be delivered in order to make it viable for the AI, ML workloads. We're talking there about a blueprint that allows you to build RDMA over ethernet or Rocky V2 based networking traffic over a lossless network using a priority flow control, using congestion control, and in combination with that, a better load balancing approach across all the available links in there. So we're looking at a proactive approach for load balancing, a reactive approach of using quality of service in order to achieve ethernet losslessness in such a network. You'll see in addition to the switching, the optics and the blueprint, you see the automation piece Nexus Dashboard Fabric Controller is allowed to give you the necessary configs for it. And then you'll see the network configuration, we're doing extensive interop testing with all the major NIC vendor, meaning how you connect your clusters to that network. And we give you a wide set of additional feature set in the NXOS operating system. I mentioned currently we're at the 64 ports of 400 gig resulting in a 25.6 terabit switching performance, switching slash routing performance. You will see the next generation silicon coming out with a 51.2 terabit, which gives you the same footprint, meaning 64 ports, but we're doubling up in the port speed all the way up to 800 gigs. So 64 ports of 800 gig is the next switching platform, which we're delivering in regards of the AI ML networking blueprint or the AI ML networking fabric. Next slide, please. Given there is a need today, we put a bundle together for you. So there is an AI ML blueprint, which is a bundle which can accommodate up to 256 GPU clusters. So you see there we are delivering a four spine, eight leaf model. If you do the math, it's actually 512 for front panel port. When we look at eight leaves, given the non-blocking approach you want, half of the ports to the front, half of the ports for the uplinks, the four spine to the interconnect. Subscription licensing is recommended, but not required. So you can tag that in. You can tag it, the additional 10% we're giving on this bundle for essential adventure, the premier licenses, as well as you get the 400 gig optics with that same bundle combination bundle discount there. You see the pit on the top right, and this bundle should be available as pair July 1st. So right for the summer vacation time or towards the next half year as we go forward. Jeremy, what else do we have other than just tables and other things? We did much more, right? Speaking of bundles and things kids want over summer break. I mean, what could be better than some next switches or large documents to read about validated designs to bring all these things together? Here we go. Both what every kid wants for summer, right? But we have the validated designs that we've been building with Nvidia, NVA, IE. Obviously we brought that on our price list as well to try and bring these solutions together so you can source it all from one place. But we're also following up with some specific rag flex pod and flashback CVDs that you'll see come into market here soon. GPT in a box. Like I mentioned earlier with Nutanix through our hyperconverged platform, additional operations for machine learning operations with Red Hat. There's more going on with Red Hat too. I think they've got a strong play in terms of how they're viewing the space. And we'll continue to develop additional capabilities out with Red Hat in our stacks. I know Lucas, your team did a lot of work with Nexus 9K and Intel Value 2 certification. So there will be a blueprint around that. And then we're going to continue to take those validated designs and build Ansible playbooks and additional automation on top of those to make it easy to go out to the hugging faces of the world to grab a model, get started. There's been a lot of work that happens after that, but at least you can get the best practices, validated design, architecture up and running as quickly as possible. And if we go to the next slide, just to close out again, I think our focus is all around building the best building blocks across these areas and then bringing them together in solutions. Nexus Hyperfabric is a great example of that. Continue to invest in validated designs that you all can take to market, help customers out with that as well. So that's it. We'll open up for some questions. Yes. Before we jump into call to action, we actually had one more question for you. So what is the difference between lossless network versus InfiniBand's lossless fabric? Are we equivalent, better or worse? Are we equivalent, better and worse? That's always a big question on where you stand on the opinion. So InfiniBand is a network technology similar like, I'll just call it the fiber channel for data networks. It was built very purpose for a very specific purpose. And the high performance computing in order to make sure that you can do memory transfers as direct and as fast as possible. There was a time where InfiniBand was outperforming Ethernet quite a bit. I think over the last couple of years, Ethernet with 400 and now 800 gig going into 1.6 terabit. We got pretty, pretty faster on that side and not only faster, but from the availability, from an operational perspective, from a knowledge perspective, from a cost per byte or per bit per second perspective, Ethernet is literally unbeatable there. And that's why we saw such a big boom around Ethernet. When we over 10 years ago started the fiber channel over Ethernet, there was like everything should be Ethernet, everything should be Ethernet at that point. So today, I think speed wise, we're comparable in that regards, Ethernet versus Ethernet. So we're comparable in that regards, Ethernet versus Ethernet versus InfiniBand. It literally ends up on the operational side, on the flexibility side. What one of my colleagues tends to say is, if you go into this call, I think we have 64 plus people in here, if I count correct, who of you can troubleshoot an InfiniBand network and probably will find one or two, but that's exactly what's out there in the industry. There is a very limited set of people that can bring in the Internet, the InfiniBand to live operate InfiniBand and get it actually working at the speed you're expecting. When we're going in this call here and ask who actually knows Ethernet, can troubleshoot Ethernet and do performance optimization on Ethernet, I probably get a thumbs up from 95 plus percent in that call. And that's where we're seeing that actually the InfiniBand train has probably left behind a bit and Ethernet ran down much more than it should have. So it's much, much faster for that specific use case and for that technology. Also, when we look at the industry, there is currently one single vendor that is doing InfiniBand, it's Nvidia. When we look at some other vendors in the AIML space like Intel or AMD, they're pretty much all swearing on Ethernet. And you can see that also that Nvidia has that, yeah, they're also good with Ethernet. Of course they are because they don't want to be left behind. So I think short answer to the very long answer I gave before, I think Ethernet outperforms InfiniBand, not just on the performance metrics on the wire, but as a whole of a technology. Ethernet ahead. Awesome. All right, questions are flowing and we have another one for you. Does this platform allow you to use Ethernet? Does it allow splitting of interfaces to support AOC cables like InfiniBand? Like Spencer may have already touched on that one. Spencer, you answered. Speak up, unmute. Oh, he's a panelist, his job is out. We want to hear him. Yeah, so there's a number of different breakout options. Right at 400 gig, you can break out to multiple 100 or 200 gig interfaces. That's correct. On the Switch platform. So there's a lot of, it offers a lot of flexibility depending on what the customer's requirements are and what types of NICs they want to connect to the fabric. So if you have any more specific questions, we can dive into those as well. Yeah, there's absolutely cable splits, breakout, absolutely there. Haven't seen it on InfiniBand. The same way, anyway. More questions. Very good, yeah, we do. Thank you, Spencer. Yep, one more. What should one expect with Total Network? What should one expect with the network cost? Nexus versus NVIDIA IB network? Oh, interesting. I think when we look at the cost of an AI cluster, where we're looking at the cost of the GPU side of the house specifically, I think the network infrastructure part is almost neglectable from a cost perspective. I think it's very comparable. I don't have a good bomb I could pull out on how much the same set of InfiniBand switches would cost, but I would say it's absolutely comparable, but we probably get more performance out of the switching and the usability, reusability of the switching versus in the InfiniBand side, meaning Ethernet versus the InfiniBand side. And I think as you look at the push away from, well, not say away, but a lot of these designs and dollars that have been spent in the market, big hyperscale, big, big, big, big, big, big use cases, right? As you start going into what enterprises are looking at and what those use cases are that they're going to solve for on a day-to-day basis, that's where Ethernet makes even more sense because you can put together a RAG architecture and worry as much about how are you going to support a thousand GPUs, you're worried about how are you going to support four or eight. And so the need for having a separate type of network to do that is really expensive at an enterprise scale. Yeah. Maybe one more thing I want to add, maybe I'll just add here, and you probably saw for those that did see the announcements at Cisco Live, Lucas and Jeremy kind of touched on this earlier. Our partnership with Nvidia, one of the key elements of that is to bring in a more mainstream Ethernet conversation into AI. And that's why we're really working together. Now, some of that work right now is coalescing around HyperFabric, but there is broader work going on where we can really kind of deepen the partnership with them across the entire portfolio on the networking side. So again, more to come on that. That's not just GPUs, it extends into GPUs and their Super NICs and so forth. So again, a lot more to come there, but they, not to speak for them, but they obviously see some of these trends on the horizon. So again, they've been really, really collaborative working with us. And you're really starting to see the fruits of that innovation present itself in these recent announcements. Yes, right. And that's actually a good segue to the question that just came in, which was around the extensibility of Nexus HyperFabric for AI to support also HGX like clusters. And so with HyperFabric for AI, we are in close collaboration with Nvidia. Again, in future, we will have HyperFabric for AI in the 25 calendar year. So it's not yet there, but the future is now. And then as we go forward with the next couple of... ...into the Nutanix on Cisco and Fusion itself. And I think you hit on it. Somebody I think is... Did you think of that? Yeah, we're just... Oh, we got him. Good. Well, it was Nutanix and... It's okay. We were talking about Cisco and Fusion. Yeah, listen to us, guys. That's a perfectly good reason to come off mute in any case. Here we go. The double WebExes. We love them. Noah said, so Cisco HyperFabric for AI is already in the future, meaning we're shipping it in the CY25, first half of the first quarter of the CY25-ish of the year. We're working right now with Nvidia to validate what kind of setups are there. There is absolute the possibility to go into... Different offerings, which Nvidia has in there like AGX at that point. But given its already future, I want to keep it till we actually ship it, and then we'll see where we go from here. Awesome. I think we'll keep it rolling. There's more questions coming in. Let's see. A comment. Worth noting, we have run into AI project storage issues with IB. Not all storage solutions support IB, but Ethernet is then needed. Yes, I guess that was Aaron, right? So let me maybe say one or two things to AI networks. There is something which is called the backend network, which is where the GPU communicates. There is a frontend network where actually the workload systems get the package loaded, then the GPU rumbles it, transports it over the backend. So in any case, Ethernet will be needed for the frontend, given everyone talks Ethernet today, while the GPU communicates. So you could, in that pocket network of what we call the backend, communicate to each other over that InfiniBand if wished. At that point, you operate two different networks with two different technologies. If you feel comfortable, that's your, I think that's a customer thing. At the end, definitely we can build backend, frontend, all with Ethernet, and we can also double up and build a storage network based on Ethernet, where we see partners like VAST or others coming in for such Ethernet-based storage systems. Sorry for the long march, but I think it was important to say that it's not just one network. Awesome. So yeah, I think we can move on to call to actions, if there are any questions at this point. Yeah, great question. Any follow-ups on top of the questions, feel free to unmute or continue to ping. First off, thank you, Lucas, Jeremy, and everyone else. And thank you, Brian, for diving deep into the new announcements. Fantastic job presenting, as always. So partners, what's next, right? How can we support you? First, we would recommend diving into our AI partner specialization. We have two newly launched partner trainings on Black Belt, both a sales and a pre-sales track. Second, an AI partner practice guide. So no matter where you're at in your AI journey, Cisco will meet you where you're at. So whether that's understanding the foundation, to building, to scaling, monetizing, this will be a great guide to help you through those steps. And then lastly, a partner sales center. So we don't want to throw different links to you. We want it in a one-stop shop place that you can find everything that you need. So both of those links, ninjas and champs, you will be the first to receive those. And then also noting, this community is open door policy, right? So any additional support, questions that you have, please, please reach out. Any suggestions for additional training are always open to get those on the calendar. So please utilize this community as much as you can. And then Maria, back over to you. Perfect. Thank you, Lindsay. So yeah, something that we also wanted to use this opportunity to cover is to talk about two brand new partner programs that we have for you. So the first one was just launched this week. It's the HIP designation. HIP is short for Hyperconverged Infrastructure Partner Designation. And it's basically the new iteration of our HX specialization. So basically the goal is to incentivize our partners to drive business with our HCI wind and chain portfolio. And obviously to increase deal velocity by receiving, as you can see on the slide, up to 85% discount on UCS M6 hardware and up to 82% on M7. So yeah, really exciting. And we have a really great program that we are hoping that you check out. And the links for everything will be available on the deck when we share after the call. We also want to give you a sneak peek on a new program launching in calendar Q3 this year, which is the Hybrid Cloud Compute Specialized Pricing Program. And the goal of this one is to reward partners who have basically completed the hybrid cloud specialization by granting up to 75% discount on HCI and HX. And we have a really great program that we are hoping to launch in the future. So stay tuned for that as well. So yeah, we really encourage you to take a look at these programs to really help boost your deal profitability. It's probably worth noting that the hybrid, I'm sorry, the hyperconverged infrastructure piece, if you are a current HX specialized partner, we will grandfather you in. Now, that's something that we're going to offer all of those partners that made that investment, and we're going to pay it forward. There will be specific details on when we're going to want you to meet the new hyperconverged infrastructure criteria. So you'll see that as it becomes available. We try to keep our terms and conditions fairly concise. But again, if you were an HX specialized partner in the past, we will grandfather you into these competitive discounts right away. Now, the only caution I'll say is that it will be gated by a deal registration. And so again, for anybody that has trouble getting that approved, just reach out to any of us. We're working very closely with our field leadership to just make sure this is a stream of thought. And we're going to try to keep it as streamlined as possible. Exactly. Brian, thanks for adding that. All right. Okay. Any other questions? Yeah. Feel free to unmute yourselves, of course, or write in the chat. So I don't see any questions coming in. Do you see any, Lindsay? I think we're good to go. Lucas, Jeremy, Brian, we're trying to hold you five minutes after. Anything we can do. Oh, actually, one just came in. Oh, there's one. Yeah. In the interest of support Cisco here, what challenges is Cisco facing that you can share? So what challenges for you guys to be have in place to push your success to the goal that you're going to achieve bycks, especially in crypto completely. Any thoughts you have on crypto? So I actually wanted to ask wondering about these, I don't know if the NASCAR forecasters need to hang out with your six engineers, or which probably are the ones that are still waiting for this rescue. I think we have to make sure we don't risk we're trying to beallows that this is what we want from you. I look at. Yeah, Lucas, you know, one question that just came to me kind of offline was around our thoughts on the UltraEthernet Consortium. I know we play a pretty key role there and the role that some of those features like packet spraying and others are going to help or support the acceleration of Ethernet in these deployments. Any thoughts there? Yeah, so UltraEthernet Consortium is basically a set of different vendors who come together and try to make Ethernet better for lossless network transport. Before, as mentioned, there was the Rocky Rocky B2 part. Now we're looking at UEC, which is doing something very similar there. There's a lot of work which is happening in the transport area, which is basically making Ethernet lossless. Which? We all know it's a collision domain. It's never really lostness from Ethernet itself, but the upper layer protocols are helping there. So there's a huge acceleration which happens there to achieve that. In conjunction to that, you mentioned better load balancing across the network. That's the first next or the first step we already achieved. They're trying to do it more load aware than the packet spraying, which has. Slightly different ideas behind. We're actually either to Nick, the DPU in that case, or the GPU in that case, the super Nick is doing the sending and the reassembly at the end where we need to attach more data to these packets that we know what the order actually was and how we sent it that we can. We can separate it on the back. So there are a lot of new ideas and technologies are coming in. The. Spring is one. We. We invested into the spring part. We looked into the spring part. We did a lot of investment in having better quality of service capabilities for RDME over converged Ethernet traffic. You see that in the blueprint. That's definitely a bigger focus there. And then, yes, as the UEC comes with the transport, we'll definitely go into that transport area. But most interestingly is a lot of the UEC work is actually. Loaded to the endpoint to the server if you want. So, and the next smartness, which comes at that with a little bit of understanding in the network and subsequently help just to come back there. There's a lot of things to basically educate people on that QoS is still cool, even as it was never cool at all. But it's literally what we do with UEC or what we do with RDME over converged. Ethernet. It's all related to better packet forwarding quality of service at the end. Hope that helps. Yeah, it does. Yeah. Thanks, Lucas. And Jeremy, maybe, maybe just one last word from you on. Do you see just in that in the context of some of the larger challenges, is there maybe a sweet spot that you would like to see the partners really kind of aim for? And I know it's kind of difficult because there's a lot of different, you know, moving pieces and opportunities. But. Is there something that you would say that is a can't miss area that that that partners should just put some additional focus around? I think if you look at our portfolio, we're really focused on the fine tuning inferencing type use cases, regardless of if that's in a modular form factor or a rack mount server. And that's where the validated designs are focused. That's where our portfolio is focused. And there's a lot of reasons we won't be. We won't probably get into it since we're running out of time. But that that is the area that I would say we would focus on. And then also we'll continue to add additional GPU type capabilities and servers and boxes as we go through the next few years. We've certainly got that feedback loud and clear. Yeah. Super. Thank you. Thank you, Jeremy. So, folks, what I would say is there's a lot more to come. And I think we're just at the tip of the iceberg as it relates to some of the announcement. Announcements that we made just recently at Cisco Live. I think there's a lot of different pieces that are ready to go right now that we could start scoping, selling. And I know several of you are already really deep in the weeds in in really positioning some of these solutions with pieces that we've already published and are ready to go. So I just want to thank you for all of you that have already leaned in. And I think for those that have been kind of. Following these announcements and know our portfolio well, please lean in because there's a lot of pieces that you'll be surprised that we could really deliver that are relevant for these AI workloads. Whether you're building purpose built or it's just another workload in your customer environment, there's a lot of innovation that we're ready to help share with you. So hopefully you got a little bit of a glimpse of that today. I just want to thank. First, Jeremy and Lucas and all of our partners who made the time some of our distributors to I saw. So thank you very much, everyone. More to come on this topic. More deep dives. And again, we'll be back here very soon. Thank you very much. And good selling out there. Thanks, everyone. Everybody. Take care. Bye bye. Thank you. Bye bye.